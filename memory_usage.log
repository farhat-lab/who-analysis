Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   103     54.7 MiB     54.7 MiB           1   @profile(stream=mem_log)
   104                                         def read_in_data():
   105                                                 
   106                                             # first get all the genotype files associated with the drug
   107     54.7 MiB      0.0 MiB           1       geno_files = []
   108                                         
   109     54.7 MiB      0.0 MiB           3       for subdir in os.listdir(os.path.join(genos_dir, f"drug_name={drug}")):
   110                                         
   111                                                 # subdirectory (tiers)
   112     54.7 MiB      0.0 MiB           2           full_subdir = os.path.join(genos_dir, f"drug_name={drug}", subdir)
   113                                         
   114                                                 # the last character is the tier number
   115     54.7 MiB      0.0 MiB           2           if full_subdir[-1] in tiers_lst:
   116     54.7 MiB      0.0 MiB           3               for fName in os.listdir(full_subdir):
   117     54.7 MiB      0.0 MiB           2                   if "run" in fName:
   118     54.7 MiB      0.0 MiB           2                       geno_files.append(os.path.join(full_subdir, fName))
   119                                         
   120     54.7 MiB      0.0 MiB           1       print(f"    {len(df_phenos)} samples with phenotypes and {len(geno_files)} files with genotypes.")
   121                                         
   122     54.7 MiB      0.0 MiB           1       dfs_lst = []
   123   3400.3 MiB   -523.4 MiB           3       for i, fName in enumerate(geno_files):
   124                                         
   125                                                 # print(f"Reading in genotypes dataframe {i+1}/{len(geno_files)}")
   126                                                 # read in the dataframe
   127   2687.7 MiB   1038.2 MiB           2           df = pd.read_csv(fName)
   128                                         
   129                                                 # get only genotypes for samples that have a phenotype
   130   2975.9 MiB   1045.1 MiB           2           df_avail_isolates = df.loc[df.sample_id.isin(df_phenos.sample_id)]
   131                                         
   132                                                 # keep all variants
   133   2975.9 MiB   -413.5 MiB           2           if synonymous:
   134                                                     dfs_lst.append(df_avail_isolates)
   135                                                 else:
   136                                                     # P = coding variants, C = synonymous or upstream variants (get only upstream variants by getting only negative positions), and N = non-coding variants on rrs/rrl
   137                                                     # deletion does not contain the p/c/n prefix
   138                                                     # synonymous variants = synonymous, change in start codon that produces V instead of M, and changes in stop codon that preserve stop
   139   3400.3 MiB    325.4 MiB           2               dfs_lst.append(df_avail_isolates.query("predicted_effect not in ['synonymous_variant', 'stop_retained_variant', 'initiator_codon_variant']"))        
   140                                         
   141                                         
   142                                             # possible to have duplicated entries because they have different predicted effects
   143                                             # example: Met1fs is present in two lines because it has 2 predicted effects: frameshift and start lost
   144                                             # sort the dataframe by inverse, which keeps start_lost before frameshift, then drop_duplicates. 
   145   3586.9 MiB    186.6 MiB           1       df_model = pd.concat(dfs_lst)
   146   4208.0 MiB    621.1 MiB           1       return df_model.sort_values("predicted_effect", ascending=False).drop_duplicates(subset=["sample_id", "resolved_symbol", "variant_category", "variant_binary_status", "variant_allele_frequency"], keep="first").reset_index(drop=True)


Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   154    739.0 MiB    739.0 MiB           1   @profile(stream=mem_log)
   155                                         def pool_lof_mutations(df):
   156                                             '''
   157                                             resolved_symbol = gene
   158                                             
   159                                             Effect = lof for ALL frameshift, nonsense, loss of start, and large-scale deletion mutations. 
   160                                             
   161                                             This function creates a new column called lof, which is 1 for variants that are lof, 0 for frameshift mutations that are not lof, and nan for variants that
   162                                             couldn't be lof (synonymous, missense, etc.)
   163                                             
   164                                             LOF criteria = loss of start or stop codon, nonsense mutation, single frameshift mutation, large-scale deletion
   165                                             
   166                                             If one of the above criteria (except the frameshift mutation) co-occurs with multiple frameshift mutations in the same sample and gene, then an lof feature will be
   167                                             generated, and the frameshift mutations will remain as additional features. i.e. the LOF will not trump the multiple frameshift mutations. 
   168                                             '''
   169                                             
   170                                             ###### STEP 1: Assign all (sample, gene) pairs with a single frameshift mutation to LOF, and the remaining to not LOF ######
   171                                             
   172                                             # get all frameshift mutations and separate by the number of frameshifts per gene per sample
   173    827.7 MiB     88.7 MiB           1       frameshift = df.query("predicted_effect == 'frameshift'")
   174                                         
   175                                             # (sample, gene) pairs with a single frameshift mutation are LOF
   176    827.7 MiB      0.0 MiB           1       lof_single_fs = pd.DataFrame(frameshift.groupby(["sample_id", "resolved_symbol"])["predicted_effect"].count()).query("predicted_effect == 1").reset_index()
   177                                         
   178                                             # already 1 because variant_category is the counts column now
   179    827.7 MiB      0.0 MiB           1       lof_single_fs.rename(columns={"predicted_effect": "lof"}, inplace=True)
   180                                         
   181                                             # lof column now is 1 for (sample, gene) pairs with only 1 frameshift mutation and 0 for those with multiple frameshift mutations
   182    827.8 MiB      0.1 MiB           1       frameshift = frameshift.merge(lof_single_fs, on=["sample_id", "resolved_symbol"], how="outer")
   183    827.9 MiB      0.1 MiB           1       frameshift["lof"] = frameshift["lof"].fillna(0)
   184                                         
   185                                             # merge with original dataframe to get the rest of the columns back. predicted_effect is now lof
   186   1626.4 MiB    798.5 MiB           1       df_with_lof = df.merge(frameshift[["sample_id", "resolved_symbol", "variant_category", "lof"]], on=["sample_id", "resolved_symbol", "variant_category"], how="outer")
   187   1626.4 MiB      0.0 MiB           1       assert len(df) == len(df_with_lof)
   188                                         
   189                                             # value_counts drops all the NaNs when computing
   190   1626.4 MiB      0.0 MiB           1       assert df_with_lof["lof"].value_counts(dropna=True).sum() == len(frameshift)
   191                                         
   192                                             ###### STEP 2: Assign loss of start, stop gained, and large-scale deletion to LOF ######
   193                                         
   194                                             # criteria for lof are: nonsense mutation, loss of start, single frameshift mutation. Get only those satisfying the first two criteria (last done above)
   195   1626.4 MiB      0.0 MiB           3       df_with_lof.loc[(df_with_lof["variant_category"] == 'deletion') | 
   196   1626.4 MiB      0.0 MiB           2                       (df_with_lof["predicted_effect"].isin(['stop_gained', 'start_lost'])), 'lof'
   197   1626.4 MiB      0.0 MiB           1                      ] = 1
   198                                             
   199                                             # get only variants that are LOF
   200   1626.4 MiB     -0.0 MiB           1       df_lof = df_with_lof.query("lof == 1")
   201                                             
   202                                             ###### STEP 3: COMBINE LOF VARIANTS WITH NON-LOF VARIANTS TO GET A FULL DATAFRAME ######
   203                                             
   204                                             # this dataframe will be slightly smaller than the original because some lof mutations have been pooled
   205                                             
   206                                             # just keep 1 instance because the feature will become just lof. The row that is kept is arbitrary
   207                                             # groupby takes more steps because the rest of the columns need to be gotten again
   208   1626.4 MiB      0.0 MiB           1       df_lof_pooled = df_lof.drop_duplicates(["sample_id", "resolved_symbol"], keep='first')
   209                                             
   210                                             # concatenate the dataframe without LOF variants with the dataframe of pooled LOF variants
   211   2424.9 MiB    798.5 MiB           1       df_final = pd.concat([df_with_lof.query("lof != 1"), df_lof_pooled], axis=0)
   212                                             
   213                                             # the lof column will now be the variant category to use, so 
   214                                             # 1. replace non-lof frame-shift mutations (value = 0) with NaN 
   215                                             # 2. replace lof variants (value = 1) with the string lof
   216                                             # 3. fill the NaNs (non-lof) with the original variant_category column
   217                                             # 4. rename columns
   218   2424.9 MiB      0.0 MiB           1       df_final["lof"] = df_final["lof"].replace(0, np.nan)
   219   2785.4 MiB    360.5 MiB           1       df_final["lof"] = df_final["lof"].replace(1, "lof")
   220   2425.4 MiB   -360.0 MiB           1       df_final["lof"] = df_final["lof"].fillna(df_final["variant_category"])
   221                                             
   222   2425.4 MiB      0.0 MiB           1       assert len(df_final["lof"].unique()) <= len(df_final["variant_category"].unique())
   223   3135.2 MiB    709.8 MiB           1       return df_final.rename(columns={"variant_category": "variant_category_unpooled", "lof": "variant_category"})


Filename: /home/sak0914/who-analysis/02_regression_with_bootstrap.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    68    458.5 MiB    458.5 MiB           1       @profile(stream=mem_log)
    69                                             def read_in_matrix_compute_grm(fName):
    70   3143.5 MiB   2685.0 MiB           1           minor_allele_counts = sparse.load_npz(fName).todense()
    71                                         
    72                                                 # convert to dataframe
    73   3143.5 MiB      0.0 MiB           1           minor_allele_counts = pd.DataFrame(minor_allele_counts)
    74   3143.5 MiB      0.0 MiB           1           minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    75   3143.5 MiB      0.0 MiB           1           minor_allele_counts = minor_allele_counts.iloc[1:, :]
    76   3143.7 MiB      0.2 MiB           1           minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    77   3143.5 MiB     -0.2 MiB           1           minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    78                                         
    79                                                 # make sample ids the index again
    80   3146.4 MiB      2.9 MiB           1           minor_allele_counts = minor_allele_counts.set_index("sample_id")
    81                                         
    82   3149.0 MiB      2.6 MiB           1           mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    83   3149.0 MiB      0.0 MiB           1           print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    84                                         
    85                                                 # compute GRM using the mino allele counts of only the samples in the model
    86   2314.5 MiB   -834.5 MiB           1           minor_allele_counts = minor_allele_counts.query("sample_id in @model_inputs.sample_id.values")
    87  11929.4 MiB   9614.9 MiB           1           grm = np.cov(minor_allele_counts.values)
    88                                         
    89  11929.4 MiB      0.0 MiB           1           minor_allele_counts_samples = minor_allele_counts.index.values
    90  10080.2 MiB  -1849.2 MiB           1           del minor_allele_counts
    91  10080.2 MiB      0.0 MiB           1           return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/02_regression_with_bootstrap.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    68    458.5 MiB    458.5 MiB           1       @profile(stream=mem_log)
    69                                             def read_in_matrix_compute_grm(fName):
    70   3143.5 MiB   2685.0 MiB           1           minor_allele_counts = sparse.load_npz(fName).todense()
    71                                         
    72                                                 # convert to dataframe
    73   3143.5 MiB      0.0 MiB           1           minor_allele_counts = pd.DataFrame(minor_allele_counts)
    74   3143.5 MiB      0.0 MiB           1           minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    75   3143.5 MiB      0.0 MiB           1           minor_allele_counts = minor_allele_counts.iloc[1:, :]
    76   3143.7 MiB      0.2 MiB           1           minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    77   3143.5 MiB     -0.2 MiB           1           minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    78                                         
    79                                                 # make sample ids the index again
    80   3146.4 MiB      2.9 MiB           1           minor_allele_counts = minor_allele_counts.set_index("sample_id")
    81                                         
    82   3149.0 MiB      2.6 MiB           1           mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    83   3149.0 MiB      0.0 MiB           1           print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    84                                         
    85                                                 # compute GRM using the mino allele counts of only the samples in the model
    86   2314.5 MiB   -834.4 MiB           1           minor_allele_counts = minor_allele_counts.query("sample_id in @model_inputs.sample_id.values")
    87  11928.2 MiB   9613.7 MiB           1           grm = np.cov(minor_allele_counts.values)
    88                                         
    89  11928.2 MiB      0.0 MiB           1           minor_allele_counts_samples = minor_allele_counts.index.values
    90  10079.0 MiB  -1849.2 MiB           1           del minor_allele_counts
    91  10079.0 MiB      0.0 MiB           1           return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/02_regression_with_bootstrap.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   198  10745.5 MiB  10745.5 MiB           1   @profile(stream=mem_log)
   199                                         def bootstrap_coef():
   200  10745.5 MiB      0.0 MiB           1       coefs = []
   201  10750.4 MiB      0.0 MiB        1001       for i in range(num_bootstrap):
   202                                         
   203                                                 # randomly draw sample indices
   204  10750.4 MiB      0.0 MiB        1000           sample_idx = np.random.choice(np.arange(0, len(y)), size=len(y), replace=True)
   205                                         
   206                                                 # get the X and y matrices
   207  11068.1 MiB 317694.1 MiB        1000           X_bs = X[sample_idx, :]
   208  11068.1 MiB    -10.7 MiB        1000           y_bs = y[sample_idx]
   209                                         
   210  11068.1 MiB    -14.2 MiB        1000           if binary:
   211  11068.1 MiB    -13.9 MiB        1000               bs_model = LogisticRegression(C=model.C_[0], penalty='l2', max_iter=10000, multi_class='ovr', class_weight='balanced')
   212                                                 else:
   213                                                     bs_model = Ridge(alpha=model.alpha_, max_iter=10000)
   214  10750.4 MiB -317776.4 MiB        1000           bs_model.fit(X_bs, y_bs)
   215  10750.4 MiB      0.0 MiB        1000           coefs.append(np.squeeze(bs_model.coef_))
   216                                         
   217  10774.4 MiB     24.1 MiB           1       return pd.DataFrame(coefs)


Filename: /home/sak0914/who-analysis/03_model_analysis.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   139     97.8 MiB     97.8 MiB           1   @profile(stream=mem_log)
   140                                         def run_all(out_dir, drug_abbr, **kwargs):
   141                                             
   142     97.8 MiB      0.0 MiB           1       tiers_lst = kwargs["tiers_lst"]
   143     97.8 MiB      0.0 MiB           1       pheno_category_lst = kwargs["pheno_category_lst"]
   144     97.8 MiB      0.0 MiB           1       model_prefix = kwargs["model_prefix"]
   145     97.8 MiB      0.0 MiB           1       het_mode = kwargs["het_mode"]
   146     97.8 MiB      0.0 MiB           1       synonymous = kwargs["synonymous"]
   147     97.8 MiB      0.0 MiB           1       pool_lof = kwargs["pool_lof"]
   148     97.8 MiB      0.0 MiB           1       AF_thresh = kwargs["AF_thresh"]
   149                                         
   150     97.8 MiB      0.0 MiB           1       num_PCs = kwargs["num_PCs"]
   151     97.8 MiB      0.0 MiB           1       num_bootstrap = kwargs["num_bootstrap"]
   152     97.8 MiB      0.0 MiB           1       alpha = kwargs["alpha"]
   153     97.8 MiB      0.0 MiB           1       binary = kwargs["binary"]
   154                                             
   155                                             # coefficients from L2 regularized regression ("baseline" regression)
   156     98.1 MiB      0.3 MiB           1       coef_df = pd.read_csv(os.path.join(out_dir, "regression_coef.csv"))
   157     98.6 MiB      0.5 MiB           1       coef_df = coef_df.query("coef != 0")
   158                                         
   159                                             # coefficients from bootstrap replicates
   160    117.0 MiB     18.4 MiB           1       bs_df = pd.read_csv(os.path.join(out_dir, "coef_bootstrap.csv"))
   161    108.1 MiB     -8.9 MiB           1       bs_df = bs_df[coef_df["variant"]]
   162                                             
   163                                             # read in all genotypes and phenotypes    
   164    110.3 MiB      2.2 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, "phenos.csv"))
   165                                         
   166                                             # add p-values and confidence intervals to the results dataframe
   167                                             # if tiers 1 and 2 are included, then compute p-values separately  
   168    110.3 MiB      0.0 MiB           1       if len(tiers_lst) > 1:
   169                                                 
   170                                                 tier1_equivalent_path = out_dir.split("tiers")[0] + "tiers=1/phenos" + out_dir.split("phenos")[-1]
   171                                                 
   172                                                 # if it's not present, then it's because this is a tiers=1+2 model with pooling LOFs. It is possible that the corresponding tiers=1, poolLOF model was not
   173                                                 # different from the tiers=1 model. So then look for that in this case. 
   174                                                 if not os.path.isdir(tier1_equivalent_path):
   175                                                     tier1_equivalent_path = out_dir.split("tiers")[0] + "tiers=1/phenos" + out_dir.split("phenos")[-1].split("_poolLOF")[0]
   176                                                 
   177                                                 tier1_matrix = pd.read_pickle(os.path.join(tier1_equivalent_path, "filt_matrix.pkl"))
   178                                                 tier1_variants = tier1_matrix.columns
   179                                                 
   180                                                 coef_df = get_pvalues_add_ci(coef_df, bs_df, "variant", len(df_phenos), tier1_variants=tier1_variants, alpha=alpha)
   181                                                 
   182                                                 # all tier 2 genes should have p-values in this case. Tier 1 p-values will be in the corresponding Tier 1 only model
   183                                                 assert len(coef_df.loc[~coef_df["variant"].isin(tier1_variants) & pd.isnull(coef_df["pval"])]) == 0
   184                                             else:
   185    117.2 MiB      6.9 MiB           1           coef_df = get_pvalues_add_ci(coef_df, bs_df, "variant", len(df_phenos), alpha=alpha)
   186                                                 
   187                                             # Benjamini-Hochberg correction
   188    117.3 MiB      0.1 MiB           1       coef_df = BH_FDR_correction(coef_df)
   189                                         
   190                                             # Bonferroni correction
   191    117.3 MiB      0.0 MiB           1       coef_df["Bonferroni_pval"] = np.min([coef_df["pval"] * len(coef_df), np.ones(len(coef_df))], axis=0)
   192                                         
   193                                             # adjusted p-values are larger so that fewer null hypotheses (coef = 0) are rejected
   194    117.3 MiB      0.0 MiB           1       assert len(coef_df.query("pval > BH_pval")) == 0
   195    117.3 MiB      0.0 MiB           1       assert len(coef_df.query("pval > Bonferroni_pval")) == 0
   196                                         
   197                                             # return all features with non-zero coefficients. Include only variants with nominally significant p-values for tractability
   198    117.3 MiB      0.0 MiB           1       coef_df = coef_df.query("pval < @alpha").sort_values("coef", ascending=False).reset_index(drop=True)
   199    117.8 MiB      0.5 MiB           1       coef_df = find_SNVs_in_current_WHO(coef_df, aa_code_dict, drug_abbr)
   200                                         
   201                                             # convert to odds ratios
   202    117.8 MiB      0.0 MiB           1       if binary:
   203    117.8 MiB      0.0 MiB           1           coef_df["Odds_Ratio"] = np.exp(coef_df["coef"])
   204    117.8 MiB      0.0 MiB           1           coef_df["OR_LB"] = np.exp(coef_df["coef_LB"])
   205    117.8 MiB      0.0 MiB           1           coef_df["OR_UB"] = np.exp(coef_df["coef_UB"])
   206                                          
   207                                             # clean up the dataframe a little -- variant and gene are from the 2021 catalog (redundant with the orig_variant column)
   208    117.8 MiB      0.0 MiB           1       del coef_df["variant"]
   209    117.8 MiB      0.0 MiB           1       del coef_df["gene"]
   210                                             #del coef_df["genome_index"]
   211                                             
   212    117.8 MiB      0.0 MiB           1       return coef_df.drop_duplicates("orig_variant", keep='first').sort_values("coef", ascending=False).reset_index(drop=True)


