Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   103     56.3 MiB     56.3 MiB           1   @profile(stream=mem_log)
   104                                         def read_in_data():
   105                                                 
   106                                             # first get all the genotype files associated with the drug
   107     56.3 MiB      0.0 MiB           1       geno_files = []
   108                                         
   109     56.3 MiB      0.0 MiB           3       for subdir in os.listdir(os.path.join(genos_dir, f"drug_name={drug}")):
   110                                         
   111                                                 # subdirectory (tiers)
   112     56.3 MiB      0.0 MiB           2           full_subdir = os.path.join(genos_dir, f"drug_name={drug}", subdir)
   113                                         
   114                                                 # the last character is the tier number
   115     56.3 MiB      0.0 MiB           2           if full_subdir[-1] in tiers_lst:
   116     56.3 MiB      0.0 MiB           8               for fName in os.listdir(full_subdir):
   117     56.3 MiB      0.0 MiB           6                   if "run" in fName:
   118     56.3 MiB      0.0 MiB           6                       geno_files.append(os.path.join(full_subdir, fName))
   119                                         
   120     56.3 MiB      0.0 MiB           1       print(f"    {len(df_phenos)} samples with phenotypes and {len(geno_files)} files with genotypes.")
   121                                         
   122     56.3 MiB      0.0 MiB           1       dfs_lst = []
   123   6408.6 MiB      0.0 MiB           7       for i, fName in enumerate(geno_files):
   124                                         
   125                                                 # print(f"Reading in genotypes dataframe {i+1}/{len(geno_files)}")
   126                                                 # read in the dataframe
   127   5260.9 MiB    530.8 MiB           6           df = pd.read_csv(fName)
   128                                         
   129                                                 # get only genotypes for samples that have a phenotype
   130   6408.6 MiB   5821.5 MiB           6           df_avail_isolates = df.loc[df.sample_id.isin(df_phenos.sample_id)]
   131                                         
   132                                                 # keep all variants
   133   6408.6 MiB      0.0 MiB           6           if synonymous:
   134   6408.6 MiB      0.0 MiB           6               dfs_lst.append(df_avail_isolates)
   135                                                 else:
   136                                                     # P = coding variants, C = synonymous or upstream variants (get only upstream variants by getting only negative positions), and N = non-coding variants on rrs/rrl
   137                                                     # deletion does not contain the p/c/n prefix
   138                                                     # synonymous variants = synonymous, change in start codon that produces V instead of M, and changes in stop codon that preserve stop
   139                                                     dfs_lst.append(df_avail_isolates.query("predicted_effect not in ['synonymous_variant', 'stop_retained_variant', 'initiator_codon_variant']"))        
   140                                         
   141                                         
   142                                             # possible to have duplicated entries because they have different predicted effects
   143                                             # example: Met1fs is present in two lines because it has 2 predicted effects: frameshift and start lost
   144                                             # sort the dataframe by inverse, which keeps start_lost before frameshift, then drop_duplicates. 
   145  11702.8 MiB   5294.2 MiB           1       df_model = pd.concat(dfs_lst)
   146  16335.5 MiB   4632.8 MiB           1       return df_model.sort_values("predicted_effect", ascending=False).drop_duplicates(subset=["sample_id", "resolved_symbol", "variant_category", "variant_binary_status", "variant_allele_frequency"], keep="first").reset_index(drop=True)


Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   154   4721.2 MiB   4721.2 MiB           1   @profile(stream=mem_log)
   155                                         def pool_lof_mutations(df):
   156                                             '''
   157                                             resolved_symbol = gene
   158                                             
   159                                             Effect = lof for ALL frameshift, nonsense, loss of start, and large-scale deletion mutations. 
   160                                             
   161                                             This function creates a new column called lof, which is 1 for variants that are lof, 0 for frameshift mutations that are not lof, and nan for variants that
   162                                             couldn't be lof (synonymous, missense, etc.)
   163                                             
   164                                             LOF criteria = loss of start or stop codon, nonsense mutation, single frameshift mutation, large-scale deletion
   165                                             
   166                                             If one of the above criteria (except the frameshift mutation) co-occurs with multiple frameshift mutations in the same sample and gene, then an lof feature will be
   167                                             generated, and the frameshift mutations will remain as additional features. i.e. the LOF will not trump the multiple frameshift mutations. 
   168                                             '''
   169                                             
   170                                             ###### STEP 1: Assign all (sample, gene) pairs with a single frameshift mutation to LOF, and the remaining to not LOF ######
   171                                             
   172                                             # get all frameshift mutations and separate by the number of frameshifts per gene per sample
   173   5501.4 MiB    780.2 MiB           1       frameshift = df.query("predicted_effect == 'frameshift'")
   174                                         
   175                                             # (sample, gene) pairs with a single frameshift mutation are LOF
   176   5504.9 MiB      3.5 MiB           1       lof_single_fs = pd.DataFrame(frameshift.groupby(["sample_id", "resolved_symbol"])["predicted_effect"].count()).query("predicted_effect == 1").reset_index()
   177                                         
   178                                             # already 1 because variant_category is the counts column now
   179   5504.9 MiB      0.0 MiB           1       lof_single_fs.rename(columns={"predicted_effect": "lof"}, inplace=True)
   180                                         
   181                                             # lof column now is 1 for (sample, gene) pairs with only 1 frameshift mutation and 0 for those with multiple frameshift mutations
   182   5600.6 MiB     95.7 MiB           1       frameshift = frameshift.merge(lof_single_fs, on=["sample_id", "resolved_symbol"], how="outer")
   183   5535.0 MiB    -65.6 MiB           1       frameshift["lof"] = frameshift["lof"].fillna(0)
   184                                         
   185                                             # merge with original dataframe to get the rest of the columns back. predicted_effect is now lof
   186  11557.4 MiB   6022.4 MiB           1       df_with_lof = df.merge(frameshift[["sample_id", "resolved_symbol", "variant_category", "lof"]], on=["sample_id", "resolved_symbol", "variant_category"], how="outer")
   187  11557.4 MiB      0.0 MiB           1       assert len(df) == len(df_with_lof)
   188                                         
   189                                             # value_counts drops all the NaNs when computing
   190  11557.4 MiB      0.0 MiB           1       assert df_with_lof["lof"].value_counts(dropna=True).sum() == len(frameshift)
   191                                         
   192                                             ###### STEP 2: Assign loss of start, stop gained, and large-scale deletion to LOF ######
   193                                         
   194                                             # criteria for lof are: nonsense mutation, loss of start, single frameshift mutation. Get only those satisfying the first two criteria (last done above)
   195  11640.2 MiB   -165.4 MiB           3       df_with_lof.loc[(df_with_lof["variant_category"] == 'deletion') | 
   196  11722.9 MiB     82.7 MiB           2                       (df_with_lof["predicted_effect"].isin(['stop_gained', 'start_lost'])), 'lof'
   197  11557.4 MiB      0.0 MiB           1                      ] = 1
   198                                             
   199                                             # get only variants that are LOF
   200  11558.0 MiB    -82.1 MiB           1       df_lof = df_with_lof.query("lof == 1")
   201                                             
   202                                             ###### STEP 3: COMBINE LOF VARIANTS WITH NON-LOF VARIANTS TO GET A FULL DATAFRAME ######
   203                                             
   204                                             # this dataframe will be slightly smaller than the original because some lof mutations have been pooled
   205                                             
   206                                             # just keep 1 instance because the feature will become just lof. The row that is kept is arbitrary
   207                                             # groupby takes more steps because the rest of the columns need to be gotten again
   208  11576.9 MiB     18.9 MiB           1       df_lof_pooled = df_lof.drop_duplicates(["sample_id", "resolved_symbol"], keep='first')
   209                                             
   210                                             # concatenate the dataframe without LOF variants with the dataframe of pooled LOF variants
   211  17484.1 MiB   5907.2 MiB           1       df_final = pd.concat([df_with_lof.query("lof != 1"), df_lof_pooled], axis=0)
   212                                             
   213                                             # the lof column will now be the variant category to use, so 
   214                                             # 1. replace non-lof frame-shift mutations (value = 0) with NaN 
   215                                             # 2. replace lof variants (value = 1) with the string lof
   216                                             # 3. fill the NaNs (non-lof) with the original variant_category column
   217                                             # 4. rename columns
   218  17484.1 MiB      0.0 MiB           1       df_final["lof"] = df_final["lof"].replace(0, np.nan)
   219  20147.9 MiB   2663.8 MiB           1       df_final["lof"] = df_final["lof"].replace(1, "lof")
   220  17484.7 MiB  -2663.2 MiB           1       df_final["lof"] = df_final["lof"].fillna(df_final["variant_category"])
   221                                             
   222  17484.7 MiB      0.0 MiB           1       assert len(df_final["lof"].unique()) <= len(df_final["variant_category"].unique())
   223  22736.3 MiB   5251.6 MiB           1       return df_final.rename(columns={"variant_category": "variant_category_unpooled", "lof": "variant_category"})


Filename: /home/sak0914/who-analysis/02_regression_with_bootstrap.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    68    773.3 MiB    773.3 MiB           1       @profile(stream=mem_log)
    69                                             def read_in_matrix_compute_grm(fName):
    70   3458.9 MiB   2685.6 MiB           1           minor_allele_counts = sparse.load_npz(fName).todense()
    71                                         
    72                                                 # convert to dataframe
    73   3458.9 MiB      0.0 MiB           1           minor_allele_counts = pd.DataFrame(minor_allele_counts)
    74   3458.9 MiB      0.0 MiB           1           minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    75   3458.9 MiB      0.0 MiB           1           minor_allele_counts = minor_allele_counts.iloc[1:, :]
    76   3459.1 MiB      0.2 MiB           1           minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    77   3458.8 MiB     -0.3 MiB           1           minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    78                                         
    79                                                 # make sample ids the index again
    80   3461.7 MiB      2.9 MiB           1           minor_allele_counts = minor_allele_counts.set_index("sample_id")
    81                                         
    82   3463.8 MiB      2.1 MiB           1           mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    83   3463.8 MiB      0.0 MiB           1           print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    84                                         
    85                                                 # compute GRM using the mino allele counts of only the samples in the model
    86   1877.4 MiB  -1586.4 MiB           1           minor_allele_counts = minor_allele_counts.query("sample_id in @model_inputs.sample_id.values")
    87   5282.2 MiB   3404.8 MiB           1           grm = np.cov(minor_allele_counts.values)
    88                                         
    89   5282.2 MiB      0.0 MiB           1           minor_allele_counts_samples = minor_allele_counts.index.values
    90   4185.8 MiB  -1096.4 MiB           1           del minor_allele_counts
    91   4185.8 MiB      0.0 MiB           1           return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/02_regression_with_bootstrap.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   195   5471.5 MiB   5471.5 MiB           1   @profile(stream=mem_log)
   196                                         def bootstrap_coef():
   197   5471.5 MiB      0.0 MiB           1       coefs = []
   198   5500.4 MiB      0.0 MiB        1001       for i in range(num_bootstrap):
   199                                         
   200                                                 # randomly draw sample indices
   201   5500.4 MiB      0.1 MiB        1000           sample_idx = np.random.choice(np.arange(0, len(y)), size=len(y), replace=True)
   202                                         
   203                                                 # get the X and y matrices
   204   6133.3 MiB 632814.4 MiB        1000           X_bs = X[sample_idx, :]
   205   6133.3 MiB      1.2 MiB        1000           y_bs = y[sample_idx]
   206                                         
   207   6133.3 MiB    -17.4 MiB        1000           if binary:
   208   6133.3 MiB    -12.8 MiB        1000               bs_model = LogisticRegression(C=model.C_[0], penalty='l2', max_iter=10000, multi_class='ovr', class_weight='balanced')
   209                                                 else:
   210                                                     bs_model = Ridge(alpha=model.alpha_, max_iter=10000)
   211   5500.4 MiB -632862.9 MiB        1000           bs_model.fit(X_bs, y_bs)
   212   5500.4 MiB      0.0 MiB        1000           coefs.append(np.squeeze(bs_model.coef_))
   213                                         
   214   5588.4 MiB     88.0 MiB           1       return pd.DataFrame(coefs)


Filename: /home/sak0914/who-analysis/03_model_analysis.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   139     98.0 MiB     98.0 MiB           1   @profile(stream=mem_log)
   140                                         def run_all(out_dir, drug_abbr, **kwargs):
   141                                             
   142     98.0 MiB      0.0 MiB           1       tiers_lst = kwargs["tiers_lst"]
   143     98.0 MiB      0.0 MiB           1       pheno_category_lst = kwargs["pheno_category_lst"]
   144     98.0 MiB      0.0 MiB           1       model_prefix = kwargs["model_prefix"]
   145     98.0 MiB      0.0 MiB           1       het_mode = kwargs["het_mode"]
   146     98.0 MiB      0.0 MiB           1       synonymous = kwargs["synonymous"]
   147     98.0 MiB      0.0 MiB           1       pool_lof = kwargs["pool_lof"]
   148     98.0 MiB      0.0 MiB           1       AF_thresh = kwargs["AF_thresh"]
   149                                         
   150     98.0 MiB      0.0 MiB           1       num_PCs = kwargs["num_PCs"]
   151     98.0 MiB      0.0 MiB           1       num_bootstrap = kwargs["num_bootstrap"]
   152     98.0 MiB      0.0 MiB           1       alpha = kwargs["alpha"]
   153     98.0 MiB      0.0 MiB           1       binary = kwargs["binary"]
   154                                             
   155                                             # coefficients from L2 regularized regression ("baseline" regression)
   156     98.9 MiB      0.9 MiB           1       coef_df = pd.read_csv(os.path.join(out_dir, "regression_coef.csv"))
   157     99.1 MiB      0.2 MiB           1       coef_df = coef_df.query("coef != 0")
   158                                         
   159                                             # coefficients from bootstrap replicates
   160    165.5 MiB     66.4 MiB           1       bs_df = pd.read_csv(os.path.join(out_dir, "coef_bootstrap.csv"))
   161    154.9 MiB    -10.5 MiB           1       bs_df = bs_df[coef_df["variant"]]
   162                                             
   163                                             # read in all genotypes and phenotypes    
   164    154.9 MiB      0.0 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, "phenos.csv"))
   165                                         
   166                                             # add p-values and confidence intervals to the results dataframe
   167                                             # if tiers 1 and 2 are included, then compute p-values separately  
   168    154.9 MiB      0.0 MiB           1       if len(tiers_lst) > 1:
   169                                                 
   170    154.9 MiB      0.0 MiB           1           tier1_equivalent_path = out_dir.split("tiers")[0] + "tiers=1/phenos" + out_dir.split("phenos")[-1]
   171                                                 
   172                                                 # if it's not present, then it's because this is a tiers=1+2 model with pooling LOFs. It is possible that the corresponding tiers=1, poolLOF model was not
   173                                                 # different from the tiers=1 model. So then look for that in this case. 
   174    154.9 MiB      0.0 MiB           1           if not os.path.isdir(tier1_equivalent_path):
   175    154.9 MiB      0.0 MiB           1               tier1_equivalent_path = out_dir.split("tiers")[0] + "tiers=1/phenos" + out_dir.split("phenos")[-1].split("_poolLOF")[0]
   176                                                 
   177    462.4 MiB    307.5 MiB           1           tier1_matrix = pd.read_pickle(os.path.join(tier1_equivalent_path, "filt_matrix.pkl"))
   178    462.4 MiB      0.0 MiB           1           tier1_variants = tier1_matrix.columns
   179                                                 
   180    481.1 MiB     18.7 MiB           1           coef_df = get_pvalues_add_ci(coef_df, bs_df, "variant", len(df_phenos), tier1_variants=tier1_variants, alpha=alpha)
   181                                                 
   182                                                 # all tier 2 genes should have p-values in this case. Tier 1 p-values will be in the corresponding Tier 1 only model
   183    481.1 MiB      0.0 MiB           1           assert len(coef_df.loc[~coef_df["variant"].isin(tier1_variants) & pd.isnull(coef_df["pval"])]) == 0
   184                                             else:
   185                                                 coef_df = get_pvalues_add_ci(coef_df, bs_df, "variant", len(df_phenos), alpha=alpha)
   186                                                 
   187                                             # Benjamini-Hochberg correction
   188    481.1 MiB      0.0 MiB           1       coef_df = BH_FDR_correction(coef_df)
   189                                         
   190                                             # Bonferroni correction
   191    481.1 MiB      0.0 MiB           1       coef_df["Bonferroni_pval"] = np.min([coef_df["pval"] * len(coef_df), np.ones(len(coef_df))], axis=0)
   192                                         
   193                                             # adjusted p-values are larger so that fewer null hypotheses (coef = 0) are rejected
   194    481.1 MiB      0.0 MiB           1       assert len(coef_df.query("pval > BH_pval")) == 0
   195    481.1 MiB      0.0 MiB           1       assert len(coef_df.query("pval > Bonferroni_pval")) == 0
   196                                         
   197                                             # return all features with non-zero coefficients. Include only variants with nominally significant p-values for tractability
   198    481.1 MiB      0.0 MiB           1       coef_df = coef_df.query("pval < @alpha").sort_values("coef", ascending=False).reset_index(drop=True)
   199    481.5 MiB      0.4 MiB           1       coef_df = find_SNVs_in_current_WHO(coef_df, aa_code_dict, drug_abbr)
   200                                         
   201                                             # convert to odds ratios
   202    481.5 MiB      0.0 MiB           1       if binary:
   203    481.5 MiB      0.0 MiB           1           coef_df["Odds_Ratio"] = np.exp(coef_df["coef"])
   204    481.5 MiB      0.0 MiB           1           coef_df["OR_LB"] = np.exp(coef_df["coef_LB"])
   205    481.5 MiB      0.0 MiB           1           coef_df["OR_UB"] = np.exp(coef_df["coef_UB"])
   206                                          
   207                                             # clean up the dataframe a little -- variant and gene are from the 2021 catalog (redundant with the orig_variant column)
   208    481.5 MiB      0.0 MiB           1       del coef_df["variant"]
   209    481.5 MiB      0.0 MiB           1       del coef_df["gene"]
   210                                             #del coef_df["genome_index"]
   211                                             
   212    481.5 MiB      0.0 MiB           1       return coef_df.drop_duplicates("orig_variant", keep='first').sort_values("coef", ascending=False).reset_index(drop=True)


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    19   1463.5 MiB   1463.5 MiB           1   @profile(stream=mem_log)
    20                                         def read_in_matrix_compute_grm(fName, samples):
    21   4149.2 MiB   2685.7 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    22                                         
    23                                             # convert to dataframe
    24   4149.2 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    25   4149.2 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    26   4149.2 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    27   4149.3 MiB      0.1 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    28   4149.0 MiB     -0.2 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    29                                         
    30                                             # make sample ids the index again
    31   4151.9 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    32                                         
    33   4154.2 MiB      2.3 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    34   4154.2 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    35                                         
    36                                             # compute GRM using the mino allele counts of only the samples in the model
    37   2566.1 MiB  -1588.1 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    38   5926.8 MiB   3360.6 MiB           1       grm = np.cov(minor_allele_counts.values)
    39                                         
    40   5926.8 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    41   4830.4 MiB  -1096.4 MiB           1       del minor_allele_counts
    42   4830.4 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    20   1464.2 MiB   1464.2 MiB           1   @profile(stream=mem_log)
    21                                         def read_in_matrix_compute_grm(fName, samples):
    22   4149.9 MiB   2685.7 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    23                                         
    24                                             # convert to dataframe
    25   4149.9 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    26   4149.9 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    27   4149.9 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    28   4149.9 MiB      0.1 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    29   4149.7 MiB     -0.2 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    30                                         
    31                                             # make sample ids the index again
    32   4152.6 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    33                                         
    34   4154.9 MiB      2.3 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    35   4154.9 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    36                                         
    37                                             # compute GRM using the mino allele counts of only the samples in the model
    38   2566.8 MiB  -1588.1 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    39   5927.4 MiB   3360.6 MiB           1       grm = np.cov(minor_allele_counts.values)
    40                                         
    41   5927.4 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    42   4831.0 MiB  -1096.4 MiB           1       del minor_allele_counts
    43   4831.0 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    20   1464.1 MiB   1464.1 MiB           1   @profile(stream=mem_log)
    21                                         def read_in_matrix_compute_grm(fName, samples):
    22   4149.8 MiB   2685.7 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    23                                         
    24                                             # convert to dataframe
    25   4149.8 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    26   4149.8 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    27   4149.8 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    28   4149.9 MiB      0.1 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    29   4149.7 MiB     -0.2 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    30                                         
    31                                             # make sample ids the index again
    32   4152.6 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    33                                         
    34   4154.9 MiB      2.3 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    35   4154.9 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    36                                         
    37                                             # compute GRM using the mino allele counts of only the samples in the model
    38   2566.8 MiB  -1588.2 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    39   5927.4 MiB   3360.6 MiB           1       grm = np.cov(minor_allele_counts.values)
    40                                         
    41   5927.4 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    42   4831.0 MiB  -1096.4 MiB           1       del minor_allele_counts
    43   4831.0 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    92    184.6 MiB    184.6 MiB           1   @profile(stream=mem_log)
    93                                         def compute_downselected_logReg_model(drug, out_dir, binary=True):
    94                                             '''
    95                                             This model computes a logistic regression model using the significant predictors from the first model.
    96                                             
    97                                             The original model was used to assign coefficients/odds ratios and p-values. Using the significant predictors (p < 0.05 after FDR), this function
    98                                             builds another L2-penalized logistic regression to compute sensitivity, specificity, AUC, accuracy, and balanced accuracy. 
    99                                             '''
   100                                             
   101                                             # final_analysis file with all significant variants for a drug
   102    185.7 MiB      1.2 MiB           1       res_df = pd.read_csv(os.path.join(out_dir, drug, "final_analysis.csv"))
   103                                             
   104                                             # read in all genotypes and phenotypes and combine into a single dataframe. 
   105                                             # Take the dataframes with the most genotypes and phenotypes represented: tiers=1+2, phenos=ALL
   106                                             # if there are significant LOF variants in res_df, then get the corresponding poolLOF matrix and combine matrices 
   107    187.2 MiB      1.5 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "phenos.csv"))
   108                                             
   109    187.5 MiB      0.2 MiB           1       if len(res_df.loc[res_df["orig_variant"].str.contains("lof")]) > 0:
   110                                                 model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   111                                                 model_inputs_poolLOF = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn_poolLOF", "filt_matrix.pkl"))
   112                                                 
   113                                                 # combine dataframes and remove duplicate columns
   114                                                 model_inputs = pd.concat([model_inputs, model_inputs_poolLOF], axis=1)
   115                                                 model_inputs = model_inputs.loc[:,~model_inputs.columns.duplicated()]
   116                                         
   117                                             else:
   118    825.8 MiB    638.3 MiB           1           model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   119                                                 
   120                                             # combine into a single dataframe and check that there are no principal components left (because there aren't in df_phenos)
   121   1464.1 MiB    638.3 MiB           1       combined = model_inputs.merge(df_phenos[["sample_id", "phenotype"]], on="sample_id", how="inner").set_index("sample_id")
   122   1464.1 MiB      0.0 MiB           1       assert sum(combined.columns.str.contains("PC")) == 0
   123                                             
   124                                             # compute GRM and get only samples that are represented in the GRM (it should be everything, but this is just to avoid errors)
   125                                             # GRM is in the order of minor_allele_counts_samples (N x N)
   126   1464.1 MiB      0.0 MiB           1       print("Reading in minor allele counts matrix and computing GRM + PCA")
   127   4831.0 MiB   3366.8 MiB           1       grm, minor_allele_counts_samples = read_in_matrix_compute_grm("data/minor_allele_counts.npz", combined.index.values)
   128   4831.0 MiB      0.0 MiB           1       combined = combined.loc[minor_allele_counts_samples, :]
   129                                             
   130   4831.0 MiB      0.0 MiB           1       scaler = StandardScaler()
   131   4831.0 MiB      0.0 MiB           1       pca = PCA(n_components=5)
   132   4846.5 MiB     15.5 MiB           1       pca.fit(scaler.fit_transform(grm))
   133                                         
   134   4846.5 MiB      0.0 MiB           1       print(f"Explained variance ratios of 5 principal components: {pca.explained_variance_ratio_}")
   135   4846.5 MiB      0.0 MiB           1       eigenvec = pca.components_.T
   136   4846.5 MiB      0.0 MiB           1       eigenvec_df = pd.DataFrame(eigenvec)
   137   4846.5 MiB      0.0 MiB           1       eigenvec_df.index = minor_allele_counts_samples
   138                                             
   139                                             # combine with eigevectors, then separate the phenotypes
   140   4846.5 MiB      0.0 MiB           1       combined = combined.merge(eigenvec_df, left_index=True, right_index=True)
   141   4846.5 MiB      0.0 MiB           1       if binary:
   142   4846.5 MiB      0.0 MiB           1           y = combined["phenotype"].values
   143   4846.5 MiB      0.0 MiB           1           del combined["phenotype"]
   144                                             
   145   5485.3 MiB    638.7 MiB           1       X = scaler.fit_transform(combined.values)
   146                                             
   147                                             # fit a regression model on the downselected data (only variants with non-zero coefficients and significant p-values after FDR)
   148   5485.3 MiB      0.0 MiB           1       if binary:
   149   5485.4 MiB      0.1 MiB           2           model = LogisticRegressionCV(Cs=np.logspace(-6, 6, 13), 
   150   5485.3 MiB      0.0 MiB           1                                        cv=5,
   151   5485.3 MiB      0.0 MiB           1                                        penalty='l2',
   152   5485.3 MiB      0.0 MiB           1                                        max_iter=10000, 
   153   5485.3 MiB      0.0 MiB           1                                        multi_class='ovr',
   154   5485.3 MiB      0.0 MiB           1                                        scoring='neg_log_loss',
   155   5485.3 MiB      0.0 MiB           1                                        class_weight='balanced'
   156                                                                             )
   157                                             else:
   158                                                 model = RidgeCV(alphas=np.logspace(-6, 6, 13),
   159                                                                 cv=5,
   160                                                                 max_iter=10000,
   161                                                                 scoring='neg_root_mean_squared_error'
   162                                                                )
   163                                             
   164                                             # fit and save the baseline model
   165   6122.9 MiB    637.6 MiB           1       model.fit(X, y)
   166   6122.9 MiB      0.0 MiB           1       if binary:
   167   6122.9 MiB      0.0 MiB           1           print(f"    Regularization parameter: {model.C_[0]}")
   168   6122.9 MiB      0.0 MiB           1           pickle.dump(model, open(os.path.join(out_dir, drug, 'logReg_model'),'wb'))
   169                                             else:
   170                                                 print(f"    Regularization parameter: {model.alpha_}")
   171                                                 pickle.dump(model, open(os.path.join(out_dir, drug, 'linReg_model'),'wb'))
   172                                         
   173                                             # get predicted probabilities. The output is N x k dimensions, where N = number of samples, and k = number of classes
   174                                             # the second column is the probability of being in the class 1, so compute the classification threshold using that
   175   6122.9 MiB      0.0 MiB           1       if binary:
   176   6122.9 MiB      0.0 MiB           1           y_proba = model.predict_proba(X)[:, 1]
   177   5485.0 MiB   -638.0 MiB           1           y_hat = get_threshold_val(y, y_proba)
   178                                         
   179                                                 # compute sensitivity, specificity, and accuracy scores (balanced and unbalanced)
   180   5485.0 MiB      0.0 MiB           1           tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y, y_hat).ravel()
   181   5485.0 MiB      0.0 MiB           1           sens = tp / (tp+fn)
   182   5485.0 MiB      0.0 MiB           1           spec = tn / (tn+fp)
   183                                         
   184                                                 # return a dataframe of the summary stats
   185   5485.0 MiB      0.0 MiB           3           return pd.DataFrame({"Sens": sens,
   186   5485.0 MiB      0.0 MiB           1                                "Spec": spec,
   187   5485.0 MiB      0.0 MiB           1                                "AUC": sklearn.metrics.roc_auc_score(y, y_hat),
   188   5485.0 MiB      0.0 MiB           1                                "F1": sklearn.metrics.f1_score(y, y_hat),
   189   5485.0 MiB      0.0 MiB           1                                "accuracy": sklearn.metrics.accuracy_score(y, y_hat),
   190   5485.0 MiB      0.0 MiB           1                                "balanced_accuracy": sklearn.metrics.balanced_accuracy_score(y, y_hat),
   191   5485.0 MiB      0.0 MiB           1                               }, index=[0]
   192                                                                    )
   193                                             else:
   194                                                 y_hat = model.predict(X)
   195                                                 mae = np.mean(np.abs((y - y_hat)))
   196                                                 rmse = np.mean((y - y_hat)**2)
   197                                                 
   198                                                 # return a dataframe of the summary stats
   199                                                 return pd.DataFrame({"MAE": mae,
   200                                                                      "RMSE": rmse,
   201                                                                     }, index=[0]
   202                                                                    )        


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    20   1464.3 MiB   1464.3 MiB           1   @profile(stream=mem_log)
    21                                         def read_in_matrix_compute_grm(fName, samples):
    22   4149.4 MiB   2685.1 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    23                                         
    24                                             # convert to dataframe
    25   4149.4 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    26   4149.4 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    27   4149.4 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    28   4149.5 MiB      0.1 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    29   4149.2 MiB     -0.3 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    30                                         
    31                                             # make sample ids the index again
    32   4152.1 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    33                                         
    34   4154.4 MiB      2.3 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    35   4154.4 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    36                                         
    37                                             # compute GRM using the mino allele counts of only the samples in the model
    38   2567.1 MiB  -1587.3 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    39   5927.7 MiB   3360.6 MiB           1       grm = np.cov(minor_allele_counts.values)
    40                                         
    41   5927.7 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    42   4831.3 MiB  -1096.4 MiB           1       del minor_allele_counts
    43   4831.3 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    20   1464.3 MiB   1464.3 MiB           1   @profile(stream=mem_log)
    21                                         def read_in_matrix_compute_grm(fName, samples):
    22   4149.4 MiB   2685.1 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    23                                         
    24                                             # convert to dataframe
    25   4149.4 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    26   4149.4 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    27   4149.4 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    28   4149.4 MiB      0.1 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    29   4149.2 MiB     -0.2 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    30                                         
    31                                             # make sample ids the index again
    32   4152.1 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    33                                         
    34   4154.4 MiB      2.3 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    35   4154.4 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    36                                         
    37                                             # compute GRM using the mino allele counts of only the samples in the model
    38   2567.1 MiB  -1587.3 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    39   5927.7 MiB   3360.6 MiB           1       grm = np.cov(minor_allele_counts.values)
    40                                         
    41   5927.7 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    42   4831.3 MiB  -1096.4 MiB           1       del minor_allele_counts
    43   4831.3 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   130    184.7 MiB    184.7 MiB           1   @profile(stream=mem_log)
   131                                         def compute_downselected_logReg_model(drug, out_dir, binary=True, num_bootstrap=1000):
   132                                             '''
   133                                             This model computes a logistic regression model using the significant predictors from the first model.
   134                                             
   135                                             The original model was used to assign coefficients/odds ratios and p-values. Using the significant predictors (p < 0.05 after FDR), this function
   136                                             builds another L2-penalized logistic regression to compute sensitivity, specificity, AUC, accuracy, and balanced accuracy. 
   137                                             '''
   138                                             
   139                                             # final_analysis file with all significant variants for a drug
   140    185.8 MiB      1.2 MiB           1       res_df = pd.read_csv(os.path.join(out_dir, drug, "final_analysis.csv"))
   141                                             
   142                                             # read in all genotypes and phenotypes and combine into a single dataframe. 
   143                                             # Take the dataframes with the most genotypes and phenotypes represented: tiers=1+2, phenos=ALL
   144                                             # if there are significant LOF variants in res_df, then get the corresponding poolLOF matrix and combine matrices 
   145    187.3 MiB      1.5 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "phenos.csv"))
   146                                             
   147    187.5 MiB      0.2 MiB           1       if len(res_df.loc[res_df["orig_variant"].str.contains("lof")]) > 0:
   148                                                 model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   149                                                 model_inputs_poolLOF = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn_poolLOF", "filt_matrix.pkl"))
   150                                                 
   151                                                 # combine dataframes and remove duplicate columns
   152                                                 model_inputs = pd.concat([model_inputs, model_inputs_poolLOF], axis=1)
   153                                                 model_inputs = model_inputs.loc[:,~model_inputs.columns.duplicated()]
   154                                         
   155                                             else:
   156    825.9 MiB    638.3 MiB           1           model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   157                                                 
   158                                             # combine into a single dataframe and check that there are no principal components left (because there aren't in df_phenos)
   159   1464.3 MiB    638.4 MiB           1       combined = model_inputs.merge(df_phenos[["sample_id", "phenotype"]], on="sample_id", how="inner").set_index("sample_id")
   160   1464.3 MiB      0.0 MiB           1       assert sum(combined.columns.str.contains("PC")) == 0
   161                                             
   162                                             # compute GRM and get only samples that are represented in the GRM (it should be everything, but this is just to avoid errors)
   163                                             # GRM is in the order of minor_allele_counts_samples (N x N)
   164   1464.3 MiB      0.0 MiB           1       print("Reading in minor allele counts matrix and computing GRM + PCA")
   165   4831.3 MiB   3367.0 MiB           1       grm, minor_allele_counts_samples = read_in_matrix_compute_grm("data/minor_allele_counts.npz", combined.index.values)
   166   4831.3 MiB      0.0 MiB           1       combined = combined.loc[minor_allele_counts_samples, :]
   167                                             
   168   4831.3 MiB      0.0 MiB           1       scaler = StandardScaler()
   169   4831.3 MiB      0.0 MiB           1       pca = PCA(n_components=5)
   170   4846.8 MiB     15.5 MiB           1       pca.fit(scaler.fit_transform(grm))
   171                                         
   172   4846.8 MiB      0.0 MiB           1       print(f"Explained variance ratios of 5 principal components: {pca.explained_variance_ratio_}")
   173   4846.8 MiB      0.0 MiB           1       eigenvec = pca.components_.T
   174   4846.8 MiB      0.0 MiB           1       eigenvec_df = pd.DataFrame(eigenvec)
   175   4846.8 MiB      0.0 MiB           1       eigenvec_df.index = minor_allele_counts_samples
   176                                             
   177                                             # combine with eigevectors, then separate the phenotypes
   178   4846.8 MiB      0.0 MiB           1       combined = combined.merge(eigenvec_df, left_index=True, right_index=True)
   179   4846.8 MiB      0.0 MiB           1       if binary:
   180   4846.8 MiB      0.0 MiB           1           y = combined["phenotype"].values
   181   4846.8 MiB      0.0 MiB           1           del combined["phenotype"]
   182                                             
   183   5485.5 MiB    638.7 MiB           1       X = scaler.fit_transform(combined.values)
   184                                             
   185                                             # fit a regression model on the downselected data (only variants with non-zero coefficients and significant p-values after FDR)
   186   5485.5 MiB      0.0 MiB           1       if binary:
   187   5485.6 MiB      0.1 MiB           2           model = LogisticRegressionCV(Cs=np.logspace(-6, 6, 13), 
   188   5485.6 MiB      0.0 MiB           1                                        cv=5,
   189   5485.6 MiB      0.0 MiB           1                                        penalty='l2',
   190   5485.6 MiB      0.0 MiB           1                                        max_iter=10000, 
   191   5485.6 MiB      0.0 MiB           1                                        multi_class='ovr',
   192   5485.6 MiB      0.0 MiB           1                                        scoring='neg_log_loss',
   193   5485.6 MiB      0.0 MiB           1                                        class_weight='balanced'
   194                                                                             )
   195                                             else:
   196                                                 model = RidgeCV(alphas=np.logspace(-6, 6, 13),
   197                                                                 cv=5,
   198                                                                 max_iter=10000,
   199                                                                 scoring='neg_root_mean_squared_error'
   200                                                                )
   201                                             
   202                                             # fit and save the baseline model if you want to make more predictions later
   203   6123.2 MiB    637.6 MiB           1       model.fit(X, y)
   204   6123.2 MiB      0.0 MiB           1       if binary:
   205   6123.2 MiB      0.0 MiB           1           print(f"    Regularization parameter: {model.C_[0]}")
   206   6123.2 MiB      0.0 MiB           1           pickle.dump(model, open(os.path.join(out_dir, drug, 'logReg_model'),'wb'))
   207                                             else:
   208                                                 print(f"    Regularization parameter: {model.alpha_}")
   209                                                 pickle.dump(model, open(os.path.join(out_dir, drug, 'linReg_model'),'wb'))
   210                                         
   211                                         
   212                                             # get the summary stats for the overall model
   213   5485.3 MiB   -637.9 MiB           1       model_outputs = generate_model_output(X, y, model, binary=True, print_thresh=True)
   214   5485.3 MiB      0.0 MiB           1       model_outputs["BS"] = 0
   215                                             
   216                                             # next, perform bootstrapping with 1000 replicates
   217   5485.3 MiB      0.0 MiB           1       print(f"Bootstrapping the summary model wiht {num_bootstrap} replicates")
   218   6123.4 MiB      0.0 MiB          11       for i in range(num_bootstrap):
   219                                         
   220                                                 # randomly draw sample indices
   221   6123.4 MiB      0.0 MiB          10           sample_idx = np.random.choice(np.arange(0, len(y)), size=len(y), replace=True)
   222                                         
   223                                                 # get the X and y matrices
   224   6761.4 MiB   6378.9 MiB          10           X_bs = X[sample_idx, :]
   225   6761.4 MiB     -0.3 MiB          10           y_bs = y[sample_idx]
   226                                         
   227   6761.4 MiB     -0.3 MiB          10           if binary:
   228   6761.4 MiB     -0.3 MiB          10               bs_model = LogisticRegression(C=model.C_[0], penalty='l2', max_iter=10000, multi_class='ovr', class_weight='balanced')
   229                                                 else:
   230                                                     bs_model = Ridge(alpha=model.alpha_, max_iter=10000)
   231                                                 
   232   6123.4 MiB  -5741.9 MiB          10           bs_model.fit(X_bs, y_bs)
   233   6123.4 MiB      0.0 MiB          10           summary = generate_model_output(X_bs, y_bs, bs_model, binary=binary, print_thresh=False)
   234   6123.4 MiB      0.0 MiB          10           summary["BS"] = 1
   235   6123.4 MiB      0.0 MiB          10           model_outputs = pd.concat([model_outputs, summary], axis=0)
   236                                         
   237   6123.4 MiB      0.0 MiB           1       return pd.DataFrame(model_outputs)


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    20   1464.3 MiB   1464.3 MiB           1   @profile(stream=mem_log)
    21                                         def read_in_matrix_compute_grm(fName, samples):
    22   4150.0 MiB   2685.7 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    23                                         
    24                                             # convert to dataframe
    25   4150.0 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    26   4150.0 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    27   4150.0 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    28   4150.1 MiB      0.1 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    29   4149.8 MiB     -0.3 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    30                                         
    31                                             # make sample ids the index again
    32   4152.7 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    33                                         
    34   4155.1 MiB      2.3 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    35   4155.1 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    36                                         
    37                                             # compute GRM using the mino allele counts of only the samples in the model
    38   2566.9 MiB  -1588.1 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    39   5951.5 MiB   3384.6 MiB           1       grm = np.cov(minor_allele_counts.values)
    40                                         
    41   5951.5 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    42   4855.1 MiB  -1096.4 MiB           1       del minor_allele_counts
    43   4855.1 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   130    184.9 MiB    184.9 MiB           1   @profile(stream=mem_log)
   131                                         def compute_downselected_logReg_model(drug, out_dir, binary=True, num_bootstrap=1000):
   132                                             '''
   133                                             This model computes a logistic regression model using the significant predictors from the first model.
   134                                             
   135                                             The original model was used to assign coefficients/odds ratios and p-values. Using the significant predictors (p < 0.05 after FDR), this function
   136                                             builds another L2-penalized logistic regression to compute sensitivity, specificity, AUC, accuracy, and balanced accuracy. 
   137                                             '''
   138                                             
   139                                             # final_analysis file with all significant variants for a drug
   140    185.9 MiB      1.0 MiB           1       res_df = pd.read_csv(os.path.join(out_dir, drug, "final_analysis.csv"))
   141                                             
   142                                             # read in all genotypes and phenotypes and combine into a single dataframe. 
   143                                             # Take the dataframes with the most genotypes and phenotypes represented: tiers=1+2, phenos=ALL
   144                                             # if there are significant LOF variants in res_df, then get the corresponding poolLOF matrix and combine matrices 
   145    187.4 MiB      1.5 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "phenos.csv"))
   146                                             
   147    187.6 MiB      0.2 MiB           1       if len(res_df.loc[res_df["orig_variant"].str.contains("lof")]) > 0:
   148                                                 model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   149                                                 model_inputs_poolLOF = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn_poolLOF", "filt_matrix.pkl"))
   150                                                 
   151                                                 # combine dataframes and remove duplicate columns
   152                                                 model_inputs = pd.concat([model_inputs, model_inputs_poolLOF], axis=1)
   153                                                 model_inputs = model_inputs.loc[:,~model_inputs.columns.duplicated()]
   154                                         
   155                                             else:
   156    826.1 MiB    638.5 MiB           1           model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   157                                                 
   158                                             # combine into a single dataframe and check that there are no principal components left (because there aren't in df_phenos)
   159   1464.3 MiB    638.2 MiB           1       combined = model_inputs.merge(df_phenos[["sample_id", "phenotype"]], on="sample_id", how="inner").set_index("sample_id")
   160   1464.3 MiB      0.0 MiB           1       assert sum(combined.columns.str.contains("PC")) == 0
   161                                             
   162                                             # compute GRM and get only samples that are represented in the GRM (it should be everything, but this is just to avoid errors)
   163                                             # GRM is in the order of minor_allele_counts_samples (N x N)
   164   4855.1 MiB   3390.8 MiB           1       grm, minor_allele_counts_samples = read_in_matrix_compute_grm("data/minor_allele_counts.npz", combined.index.values)
   165   4855.1 MiB      0.0 MiB           1       combined = combined.loc[minor_allele_counts_samples, :]
   166                                             
   167   4855.1 MiB      0.0 MiB           1       scaler = StandardScaler()
   168   4855.1 MiB      0.0 MiB           1       pca = PCA(n_components=5)
   169   4870.9 MiB     15.8 MiB           1       pca.fit(scaler.fit_transform(grm))
   170                                         
   171   4870.9 MiB      0.0 MiB           1       print(f"Explained variance ratios of 5 principal components: {pca.explained_variance_ratio_}")
   172   4870.9 MiB      0.0 MiB           1       eigenvec = pca.components_.T
   173   4870.9 MiB      0.0 MiB           1       eigenvec_df = pd.DataFrame(eigenvec)
   174   4870.9 MiB      0.0 MiB           1       eigenvec_df.index = minor_allele_counts_samples
   175                                             
   176                                             # combine with eigevectors, then separate the phenotypes
   177   4870.9 MiB      0.0 MiB           1       combined = combined.merge(eigenvec_df, left_index=True, right_index=True)
   178   4870.9 MiB      0.0 MiB           1       if binary:
   179   4870.9 MiB      0.0 MiB           1           y = combined["phenotype"].values
   180   4870.9 MiB      0.0 MiB           1           del combined["phenotype"]
   181                                             
   182   5509.5 MiB    638.6 MiB           1       X = scaler.fit_transform(combined.values)
   183                                             
   184                                             # fit a regression model on the downselected data (only variants with non-zero coefficients and significant p-values after FDR)
   185   5509.5 MiB      0.0 MiB           1       if binary:
   186   5509.7 MiB      0.2 MiB           2           model = LogisticRegressionCV(Cs=np.logspace(-6, 6, 13), 
   187   5509.7 MiB      0.0 MiB           1                                        cv=5,
   188   5509.7 MiB      0.0 MiB           1                                        penalty='l2',
   189   5509.7 MiB      0.0 MiB           1                                        max_iter=10000, 
   190   5509.7 MiB      0.0 MiB           1                                        multi_class='ovr',
   191   5509.7 MiB      0.0 MiB           1                                        scoring='neg_log_loss',
   192   5509.7 MiB      0.0 MiB           1                                        class_weight='balanced'
   193                                                                             )
   194                                             else:
   195                                                 model = RidgeCV(alphas=np.logspace(-6, 6, 13),
   196                                                                 cv=5,
   197                                                                 max_iter=10000,
   198                                                                 scoring='neg_root_mean_squared_error'
   199                                                                )
   200                                             
   201                                             # fit and save the baseline model if you want to make more predictions later
   202   6147.5 MiB    637.8 MiB           1       model.fit(X, y)
   203   6147.5 MiB      0.0 MiB           1       if binary:
   204   6147.5 MiB      0.0 MiB           1           print(f"    Regularization parameter: {model.C_[0]}")
   205   6147.5 MiB      0.0 MiB           1           pickle.dump(model, open(os.path.join(out_dir, drug, 'logReg_model'),'wb'))
   206                                             else:
   207                                                 print(f"    Regularization parameter: {model.alpha_}")
   208                                                 pickle.dump(model, open(os.path.join(out_dir, drug, 'linReg_model'),'wb'))
   209                                         
   210                                         
   211                                             # get the summary stats for the overall model
   212   5509.6 MiB   -637.9 MiB           1       model_outputs = generate_model_output(X, y, model, binary=True, print_thresh=True)
   213   5509.6 MiB      0.0 MiB           1       model_outputs["BS"] = 0
   214                                             
   215                                             # next, perform bootstrapping with 1000 replicates
   216   5509.6 MiB      0.0 MiB           1       print(f"Bootstrapping the summary model with {num_bootstrap} replicates")
   217   6147.9 MiB      0.0 MiB        1001       for i in range(num_bootstrap):
   218                                         
   219                                                 # randomly draw sample indices
   220   6147.9 MiB      0.1 MiB        1000           sample_idx = np.random.choice(np.arange(0, len(y)), size=len(y), replace=True)
   221                                         
   222                                                 # get the X and y matrices
   223   6785.9 MiB 623897.0 MiB        1000           X_bs = X[sample_idx, :]
   224   6785.9 MiB -14056.5 MiB        1000           y_bs = y[sample_idx]
   225                                         
   226   6785.9 MiB -14056.5 MiB        1000           if binary:
   227   6785.9 MiB -14050.9 MiB        1000               bs_model = LogisticRegression(C=model.C_[0], penalty='l2', max_iter=10000, multi_class='ovr', class_weight='balanced')
   228                                                 else:
   229                                                     bs_model = Ridge(alpha=model.alpha_, max_iter=10000)
   230                                                 
   231   6147.9 MiB -637373.2 MiB        1000           bs_model.fit(X_bs, y_bs)
   232   6147.9 MiB      0.0 MiB        1000           summary = generate_model_output(X_bs, y_bs, bs_model, binary=binary, print_thresh=False)
   233   6147.9 MiB      0.0 MiB        1000           summary["BS"] = 1
   234   6147.9 MiB      0.0 MiB        1000           model_outputs = pd.concat([model_outputs, summary], axis=0)
   235                                         
   236   6147.9 MiB      0.0 MiB           1       return pd.DataFrame(model_outputs)


