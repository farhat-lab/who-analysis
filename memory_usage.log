Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    99     56.3 MiB     56.3 MiB           1   @profile(stream=mem_log)
   100                                         def read_in_data():
   101                                                 
   102                                             # first get all the genotype files associated with the drug
   103     56.3 MiB      0.0 MiB           1       geno_files = []
   104                                         
   105     56.3 MiB      0.0 MiB           3       for subdir in os.listdir(os.path.join(genos_dir, f"drug_name={drug}")):
   106                                         
   107                                                 # subdirectory (tiers)
   108     56.3 MiB      0.0 MiB           2           full_subdir = os.path.join(genos_dir, f"drug_name={drug}", subdir)
   109                                         
   110                                                 # the last character is the tier number
   111     56.3 MiB      0.0 MiB           2           if full_subdir[-1] in tiers_lst:
   112     56.3 MiB      0.0 MiB           3               for fName in os.listdir(full_subdir):
   113     56.3 MiB      0.0 MiB           2                   if "run" in fName:
   114     56.3 MiB      0.0 MiB           2                       geno_files.append(os.path.join(full_subdir, fName))
   115                                         
   116     56.3 MiB      0.0 MiB           1       print(f"    {len(df_phenos)} samples with phenotypes and {len(geno_files)} files with genotypes.")
   117                                         
   118     56.3 MiB      0.0 MiB           1       dfs_lst = []
   119   3575.2 MiB      0.0 MiB           3       for i, fName in enumerate(geno_files):
   120                                         
   121                                                 # print(f"Reading in genotypes dataframe {i+1}/{len(geno_files)}")
   122                                                 # read in the dataframe
   123   2429.0 MiB    736.3 MiB           2           df = pd.read_csv(fName)
   124                                         
   125                                                 # get only genotypes for samples that have a phenotype
   126   3575.2 MiB   2782.7 MiB           2           df_avail_isolates = df.loc[df.sample_id.isin(df_phenos.sample_id)]
   127                                         
   128                                                 # keep all variants
   129   3575.2 MiB      0.0 MiB           2           if synonymous:
   130   3575.2 MiB      0.0 MiB           2               dfs_lst.append(df_avail_isolates)
   131                                                 else:
   132                                                     # P = coding variants, C = synonymous or upstream variants (get only upstream variants by getting only negative positions), and N = non-coding variants on rrs/rrl
   133                                                     # deletion does not contain the p/c/n prefix
   134                                                     # synonymous variants = synonymous, change in start codon that produces V instead of M, and changes in stop codon that preserve stop
   135                                                     dfs_lst.append(df_avail_isolates.query("predicted_effect not in ['synonymous_variant', 'stop_retained_variant', 'initiator_codon_variant']"))        
   136                                         
   137                                         
   138                                             # possible to have duplicated entries because they have different predicted effects
   139                                             # example: Met1fs is present in two lines because it has 2 predicted effects: frameshift and start lost
   140                                             # sort the dataframe by inverse, which keeps start_lost before frameshift, then drop_duplicates. 
   141   6036.2 MiB   2461.1 MiB           1       df_model = pd.concat(dfs_lst)
   142   8189.9 MiB   2153.6 MiB           1       return df_model.sort_values("predicted_effect", ascending=False).drop_duplicates(subset=["sample_id", "resolved_symbol", "variant_category", "variant_binary_status", "variant_allele_frequency"], keep="first").reset_index(drop=True)


