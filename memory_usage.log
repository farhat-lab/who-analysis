Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   103     55.2 MiB     55.2 MiB           1   @profile(stream=mem_log)
   104                                         def read_in_data():
   105                                                 
   106                                             # first get all the genotype files associated with the drug
   107     55.2 MiB      0.0 MiB           1       geno_files = []
   108                                         
   109     55.2 MiB      0.0 MiB           3       for subdir in os.listdir(os.path.join(genos_dir, f"drug_name={drug}")):
   110                                         
   111                                                 # subdirectory (tiers)
   112     55.2 MiB      0.0 MiB           2           full_subdir = os.path.join(genos_dir, f"drug_name={drug}", subdir)
   113                                         
   114                                                 # the last character is the tier number
   115     55.2 MiB      0.0 MiB           2           if full_subdir[-1] in tiers_lst:
   116     55.2 MiB      0.0 MiB          11               for fName in os.listdir(full_subdir):
   117     55.2 MiB      0.0 MiB           9                   if "run" in fName:
   118     55.2 MiB      0.0 MiB           9                       geno_files.append(os.path.join(full_subdir, fName))
   119                                         
   120     55.2 MiB      0.0 MiB           1       print(f"    {len(df_phenos)} samples with phenotypes and {len(geno_files)} files with genotypes.")
   121                                         
   122     55.2 MiB      0.0 MiB           1       dfs_lst = []
   123  22931.6 MiB -31238.7 MiB          10       for i, fName in enumerate(geno_files):
   124                                         
   125                                                 # print(f"Reading in genotypes dataframe {i+1}/{len(geno_files)}")
   126                                                 # read in the dataframe
   127  15884.5 MiB -28684.9 MiB           9           df = pd.read_csv(fName)
   128                                         
   129                                                 # get only genotypes for samples that have a phenotype
   130  20392.4 MiB   -447.4 MiB           9           df_avail_isolates = df.loc[df.sample_id.isin(df_phenos.sample_id)]
   131                                         
   132                                                 # keep all variants
   133  20392.4 MiB -22781.7 MiB           9           if synonymous:
   134                                                     dfs_lst.append(df_avail_isolates)
   135                                                 else:
   136                                                     # P = coding variants, C = synonymous or upstream variants (get only upstream variants by getting only negative positions), and N = non-coding variants on rrs/rrl
   137                                                     # deletion does not contain the p/c/n prefix
   138                                                     # synonymous variants = synonymous, change in start codon that produces V instead of M, and changes in stop codon that preserve stop
   139  22931.6 MiB -10663.4 MiB           9               dfs_lst.append(df_avail_isolates.query("predicted_effect not in ['synonymous_variant', 'stop_retained_variant', 'initiator_codon_variant']"))        
   140                                         
   141                                         
   142                                             # possible to have duplicated entries because they have different predicted effects
   143                                             # example: Met1fs is present in two lines because it has 2 predicted effects: frameshift and start lost
   144                                             # sort the dataframe by inverse, which keeps start_lost before frameshift, then drop_duplicates. 
   145  35045.3 MiB  12113.7 MiB           1       df_model = pd.concat(dfs_lst)
   146  45643.3 MiB  10598.0 MiB           1       return df_model.sort_values("predicted_effect", ascending=False).drop_duplicates(subset=["sample_id", "resolved_symbol", "variant_category", "variant_binary_status", "variant_allele_frequency"], keep="first").reset_index(drop=True)


Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   154  10703.1 MiB  10703.1 MiB           1   @profile(stream=mem_log)
   155                                         def pool_lof_mutations(df):
   156                                             '''
   157                                             resolved_symbol = gene
   158                                             
   159                                             Effect = lof for ALL frameshift, nonsense, loss of start, and large-scale deletion mutations. 
   160                                             
   161                                             This function creates a new column called lof, which is 1 for variants that are lof, 0 for frameshift mutations that are not lof, and nan for variants that
   162                                             couldn't be lof (synonymous, missense, etc.)
   163                                             
   164                                             LOF criteria = loss of start or stop codon, nonsense mutation, single frameshift mutation, large-scale deletion
   165                                             
   166                                             If one of the above criteria (except the frameshift mutation) co-occurs with multiple frameshift mutations in the same sample and gene, then an lof feature will be
   167                                             generated, and the frameshift mutations will remain as additional features. i.e. the LOF will not trump the multiple frameshift mutations. 
   168                                             '''
   169                                             
   170                                             ###### STEP 1: Assign all (sample, gene) pairs with a single frameshift mutation to LOF, and the remaining to not LOF ######
   171                                             
   172                                             # get all frameshift mutations and separate by the number of frameshifts per gene per sample
   173  12539.9 MiB   1836.7 MiB           1       frameshift = df.query("predicted_effect == 'frameshift'")
   174                                         
   175                                             # (sample, gene) pairs with a single frameshift mutation are LOF
   176  12586.5 MiB     46.7 MiB           1       lof_single_fs = pd.DataFrame(frameshift.groupby(["sample_id", "resolved_symbol"])["predicted_effect"].count()).query("predicted_effect == 1").reset_index()
   177                                         
   178                                             # already 1 because variant_category is the counts column now
   179  12586.5 MiB      0.0 MiB           1       lof_single_fs.rename(columns={"predicted_effect": "lof"}, inplace=True)
   180                                         
   181                                             # lof column now is 1 for (sample, gene) pairs with only 1 frameshift mutation and 0 for those with multiple frameshift mutations
   182  12632.9 MiB     46.4 MiB           1       frameshift = frameshift.merge(lof_single_fs, on=["sample_id", "resolved_symbol"], how="outer")
   183  12632.9 MiB      0.0 MiB           1       frameshift["lof"] = frameshift["lof"].fillna(0)
   184                                         
   185                                             # merge with original dataframe to get the rest of the columns back. predicted_effect is now lof
   186  26268.0 MiB  13635.1 MiB           1       df_with_lof = df.merge(frameshift[["sample_id", "resolved_symbol", "variant_category", "lof"]], on=["sample_id", "resolved_symbol", "variant_category"], how="outer")
   187  26268.0 MiB      0.0 MiB           1       assert len(df) == len(df_with_lof)
   188                                         
   189                                             # value_counts drops all the NaNs when computing
   190  26268.0 MiB      0.0 MiB           1       assert df_with_lof["lof"].value_counts(dropna=True).sum() == len(frameshift)
   191                                         
   192                                             ###### STEP 2: Assign loss of start, stop gained, and large-scale deletion to LOF ######
   193                                         
   194                                             # criteria for lof are: nonsense mutation, loss of start, single frameshift mutation. Get only those satisfying the first two criteria (last done above)
   195  26457.4 MiB   -378.5 MiB           3       df_with_lof.loc[(df_with_lof["variant_category"] == 'deletion') | 
   196  26646.6 MiB    189.3 MiB           2                       (df_with_lof["predicted_effect"].isin(['stop_gained', 'start_lost'])), 'lof'
   197  26268.0 MiB      0.0 MiB           1                      ] = 1
   198                                             
   199                                             # get only variants that are LOF
   200  26375.3 MiB    -82.0 MiB           1       df_lof = df_with_lof.query("lof == 1")
   201                                             
   202                                             ###### STEP 3: COMBINE LOF VARIANTS WITH NON-LOF VARIANTS TO GET A FULL DATAFRAME ######
   203                                             
   204                                             # this dataframe will be slightly smaller than the original because some lof mutations have been pooled
   205                                             
   206                                             # just keep 1 instance because the feature will become just lof. The row that is kept is arbitrary
   207                                             # groupby takes more steps because the rest of the columns need to be gotten again
   208  26387.0 MiB     11.7 MiB           1       df_lof_pooled = df_lof.drop_duplicates(["sample_id", "resolved_symbol"], keep='first')
   209                                             
   210                                             # concatenate the dataframe without LOF variants with the dataframe of pooled LOF variants
   211  39896.2 MiB  13509.2 MiB           1       df_final = pd.concat([df_with_lof.query("lof != 1"), df_lof_pooled], axis=0)
   212                                             
   213                                             # the lof column will now be the variant category to use, so 
   214                                             # 1. replace non-lof frame-shift mutations (value = 0) with NaN 
   215                                             # 2. replace lof variants (value = 1) with the string lof
   216                                             # 3. fill the NaNs (non-lof) with the original variant_category column
   217                                             # 4. rename columns
   218  39896.2 MiB      0.0 MiB           1       df_final["lof"] = df_final["lof"].replace(0, np.nan)
   219  45986.4 MiB   6090.2 MiB           1       df_final["lof"] = df_final["lof"].replace(1, "lof")
   220  39896.9 MiB  -6089.5 MiB           1       df_final["lof"] = df_final["lof"].fillna(df_final["variant_category"])
   221                                             
   222  39903.5 MiB      6.7 MiB           1       assert len(df_final["lof"].unique()) <= len(df_final["variant_category"].unique())
   223  51911.0 MiB  12007.4 MiB           1       return df_final.rename(columns={"variant_category": "variant_category_unpooled", "lof": "variant_category"})


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    20   1434.5 MiB   1434.5 MiB           1   @profile(stream=mem_log)
    21                                         def read_in_matrix_compute_grm(fName, samples):
    22   4119.7 MiB   2685.2 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    23                                         
    24                                             # convert to dataframe
    25   4119.7 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    26   4119.7 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    27   4119.7 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    28   4119.8 MiB      0.1 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    29   4119.5 MiB     -0.3 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    30                                         
    31                                             # make sample ids the index again
    32   4122.4 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    33                                         
    34   4124.7 MiB      2.3 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    35   4124.7 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    36                                         
    37                                             # compute GRM using the mino allele counts of only the samples in the model
    38   2537.3 MiB  -1587.4 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    39   5932.4 MiB   3395.1 MiB           1       grm = np.cov(minor_allele_counts.values)
    40                                         
    41   5932.4 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    42   4836.0 MiB  -1096.4 MiB           1       del minor_allele_counts
    43   4836.0 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   130    154.8 MiB    154.8 MiB           1   @profile(stream=mem_log)
   131                                         def compute_downselected_logReg_model(drug, out_dir, binary=True, num_bootstrap=1000):
   132                                             '''
   133                                             This model computes a logistic regression model using the significant predictors from the first model.
   134                                             
   135                                             The original model was used to assign coefficients/odds ratios and p-values. Using the significant predictors (p < 0.05 after FDR), this function
   136                                             builds another L2-penalized logistic regression to compute sensitivity, specificity, AUC, accuracy, and balanced accuracy. 
   137                                             '''
   138                                             
   139                                             # final_analysis file with all significant variants for a drug
   140    158.2 MiB      3.5 MiB           1       res_df = pd.read_csv(os.path.join(out_dir, drug, "final_analysis.csv"))
   141                                             
   142                                             # # get only significant variants: 0.05 for core features, 0.01 for the rest
   143                                             # res_df = res_df.query("(Tier1_only == 1 & WHO_phenos == 1 & poolLOF == 1 & Syn == 0 & BH_pval < 0.05) | (~(Tier1_only == 1 & WHO_phenos == 1 & poolLOF == 1 & Syn == 0) & BH_pval < 0.01)")
   144    158.6 MiB      0.4 MiB           1       res_df = res_df.query("(coef_LB > 0 & coef_UB > 0) | (coef_LB < 0 & coef_UB < 0)")
   145                                         
   146                                             # read in all genotypes and phenotypes and combine into a single dataframe. 
   147                                             # Take the dataframes with the most genotypes and phenotypes represented: tiers=1+2, phenos=ALL
   148                                             # if there are significant LOF variants in res_df, then get the corresponding poolLOF matrix and combine matrices 
   149    158.7 MiB      0.1 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "phenos.csv"))
   150                                             
   151    158.7 MiB      0.0 MiB           1       if len(res_df.loc[res_df["orig_variant"].str.contains("lof")]) > 0:
   152                                                 model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   153                                                 model_inputs_poolLOF = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn_poolLOF", "filt_matrix.pkl"))
   154                                                 
   155                                                 # combine dataframes and remove duplicate columns
   156                                                 model_inputs = pd.concat([model_inputs, model_inputs_poolLOF], axis=1)
   157                                                 model_inputs = model_inputs.loc[:,~model_inputs.columns.duplicated()]
   158                                         
   159                                             else:
   160    796.4 MiB    637.7 MiB           1           model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   161                                                 
   162                                             # combine into a single dataframe and check that there are no principal components left (because there aren't in df_phenos)
   163   1434.5 MiB    638.1 MiB           1       combined = model_inputs.merge(df_phenos[["sample_id", "phenotype"]], on="sample_id", how="inner").set_index("sample_id")
   164   1434.5 MiB      0.0 MiB           1       assert sum(combined.columns.str.contains("PC")) == 0
   165                                             
   166                                             # compute GRM and get only samples that are represented in the GRM (it should be everything, but this is just to avoid errors)
   167                                             # GRM is in the order of minor_allele_counts_samples (N x N)
   168   4836.0 MiB   3401.5 MiB           1       grm, minor_allele_counts_samples = read_in_matrix_compute_grm("data/minor_allele_counts.npz", combined.index.values)
   169   4836.0 MiB      0.0 MiB           1       combined = combined.loc[minor_allele_counts_samples, :]
   170                                             
   171   4836.0 MiB      0.0 MiB           1       scaler = StandardScaler()
   172   4836.0 MiB      0.0 MiB           1       pca = PCA(n_components=5)
   173   4843.6 MiB      7.6 MiB           1       pca.fit(scaler.fit_transform(grm))
   174                                         
   175   4843.6 MiB      0.0 MiB           1       print(f"Explained variance ratios of 5 principal components: {pca.explained_variance_ratio_}")
   176   4843.6 MiB      0.0 MiB           1       eigenvec = pca.components_.T
   177   4843.6 MiB      0.0 MiB           1       eigenvec_df = pd.DataFrame(eigenvec)
   178   4843.6 MiB      0.0 MiB           1       eigenvec_df.index = minor_allele_counts_samples
   179                                             
   180                                             # combine with eigevectors, then separate the phenotypes
   181   4843.6 MiB      0.0 MiB           1       combined = combined.merge(eigenvec_df, left_index=True, right_index=True)
   182   4843.6 MiB      0.0 MiB           1       if binary:
   183   4843.6 MiB      0.0 MiB           1           y = combined["phenotype"].values
   184   4843.6 MiB      0.0 MiB           1           del combined["phenotype"]
   185                                             
   186   5482.3 MiB    638.7 MiB           1       X = scaler.fit_transform(combined.values)
   187                                             
   188                                             # fit a regression model on the downselected data (only variants with non-zero coefficients and significant p-values after FDR)
   189   5482.3 MiB      0.0 MiB           1       if binary:
   190   5482.4 MiB      0.1 MiB           2           model = LogisticRegressionCV(Cs=np.logspace(-6, 6, 13), 
   191   5482.3 MiB      0.0 MiB           1                                        cv=5,
   192   5482.3 MiB      0.0 MiB           1                                        penalty='l2',
   193   5482.3 MiB      0.0 MiB           1                                        max_iter=10000, 
   194   5482.3 MiB      0.0 MiB           1                                        multi_class='ovr',
   195   5482.3 MiB      0.0 MiB           1                                        scoring='neg_log_loss',
   196   5482.3 MiB      0.0 MiB           1                                        class_weight='balanced'
   197                                                                             )
   198                                             else:
   199                                                 model = RidgeCV(alphas=np.logspace(-6, 6, 13),
   200                                                                 cv=5,
   201                                                                 max_iter=10000,
   202                                                                 scoring='neg_root_mean_squared_error'
   203                                                                )
   204                                             
   205                                             # fit and save the baseline model if you want to make more predictions later
   206   6121.8 MiB    639.4 MiB           1       model.fit(X, y)
   207   6121.8 MiB      0.0 MiB           1       if binary:
   208   6121.8 MiB      0.0 MiB           1           print(f"    Regularization parameter: {model.C_[0]}")
   209   6121.8 MiB      0.0 MiB           1           pickle.dump(model, open(os.path.join(out_dir, drug, 'logReg_model'),'wb'))
   210                                             else:
   211                                                 print(f"    Regularization parameter: {model.alpha_}")
   212                                                 pickle.dump(model, open(os.path.join(out_dir, drug, 'linReg_model'),'wb'))
   213                                         
   214                                         
   215                                             # get the summary stats for the overall model
   216   5483.9 MiB   -637.9 MiB           1       model_outputs = generate_model_output(X, y, model, binary=True, print_thresh=True)
   217   5483.9 MiB      0.0 MiB           1       model_outputs["BS"] = 0
   218                                             
   219                                         #     # next, perform bootstrapping with 1000 replicates
   220                                         #     print(f"Bootstrapping the summary model with {num_bootstrap} replicates")
   221                                         #     for i in range(num_bootstrap):
   222                                         
   223                                         #         # randomly draw sample indices
   224                                         #         sample_idx = np.random.choice(np.arange(0, len(y)), size=len(y), replace=True)
   225                                         
   226                                         #         # get the X and y matrices
   227                                         #         X_bs = X[sample_idx, :]
   228                                         #         y_bs = y[sample_idx]
   229                                         
   230                                         #         if binary:
   231                                         #             bs_model = LogisticRegression(C=model.C_[0], penalty='l2', max_iter=10000, multi_class='ovr', class_weight='balanced')
   232                                         #         else:
   233                                         #             bs_model = Ridge(alpha=model.alpha_, max_iter=10000)
   234                                                 
   235                                         #         bs_model.fit(X_bs, y_bs)
   236                                         #         summary = generate_model_output(X_bs, y_bs, bs_model, binary=binary, print_thresh=False)
   237                                         #         summary["BS"] = 1
   238                                         #         model_outputs = pd.concat([model_outputs, summary], axis=0)
   239                                                 
   240                                         #         if i % (num_bootstrap / 10) == 0:
   241                                         #             print(i)
   242                                         
   243   5483.9 MiB      0.0 MiB           1       return pd.DataFrame(model_outputs)


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    20   1939.9 MiB   1939.9 MiB           1   @profile(stream=mem_log)
    21                                         def read_in_matrix_compute_grm(fName, samples):
    22   4624.8 MiB   2684.9 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    23                                         
    24                                             # convert to dataframe
    25   4624.8 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    26   4624.8 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    27   4624.8 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    28   4624.8 MiB      0.0 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    29   4624.6 MiB     -0.2 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    30                                         
    31                                             # make sample ids the index again
    32   4627.5 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    33                                         
    34   4629.6 MiB      2.1 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    35   4629.6 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    36                                         
    37                                             # compute GRM using the mino allele counts of only the samples in the model
    38   3329.2 MiB  -1300.4 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    39   8725.3 MiB   5396.1 MiB           1       grm = np.cov(minor_allele_counts.values)
    40                                         
    41   8725.3 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    42   7341.2 MiB  -1384.1 MiB           1       del minor_allele_counts
    43   7341.2 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   130    154.8 MiB    154.8 MiB           1   @profile(stream=mem_log)
   131                                         def compute_downselected_logReg_model(drug, out_dir, binary=True, num_bootstrap=1000):
   132                                             '''
   133                                             This model computes a logistic regression model using the significant predictors from the first model.
   134                                             
   135                                             The original model was used to assign coefficients/odds ratios and p-values. Using the significant predictors (p < 0.05 after FDR), this function
   136                                             builds another L2-penalized logistic regression to compute sensitivity, specificity, AUC, accuracy, and balanced accuracy. 
   137                                             '''
   138                                             
   139                                             # final_analysis file with all significant variants for a drug
   140    158.6 MiB      3.8 MiB           1       res_df = pd.read_csv(os.path.join(out_dir, drug, "final_analysis.csv"))
   141                                             
   142                                             # # get only significant variants: 0.05 for core features, 0.01 for the rest
   143                                             # res_df = res_df.query("(Tier1_only == 1 & WHO_phenos == 1 & poolLOF == 1 & Syn == 0 & BH_pval < 0.05) | (~(Tier1_only == 1 & WHO_phenos == 1 & poolLOF == 1 & Syn == 0) & BH_pval < 0.01)")
   144    159.0 MiB      0.4 MiB           1       res_df = res_df.query("(coef_LB > 0 & coef_UB > 0) | (coef_LB < 0 & coef_UB < 0)")
   145                                         
   146                                             # read in all genotypes and phenotypes and combine into a single dataframe. 
   147                                             # Take the dataframes with the most genotypes and phenotypes represented: tiers=1+2, phenos=ALL
   148                                             # if there are significant LOF variants in res_df, then get the corresponding poolLOF matrix and combine matrices 
   149    159.1 MiB      0.1 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "phenos.csv"))
   150                                             
   151    159.1 MiB      0.0 MiB           1       if len(res_df.loc[res_df["orig_variant"].str.contains("lof")]) > 0:
   152                                                 model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   153                                                 model_inputs_poolLOF = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn_poolLOF", "filt_matrix.pkl"))
   154                                                 
   155                                                 # combine dataframes and remove duplicate columns
   156                                                 model_inputs = pd.concat([model_inputs, model_inputs_poolLOF], axis=1)
   157                                                 model_inputs = model_inputs.loc[:,~model_inputs.columns.duplicated()]
   158                                         
   159                                             else:
   160   1049.0 MiB    889.9 MiB           1           model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   161                                                 
   162                                             # combine into a single dataframe and check that there are no principal components left (because there aren't in df_phenos)
   163   1939.9 MiB    890.9 MiB           1       combined = model_inputs.merge(df_phenos[["sample_id", "phenotype"]], on="sample_id", how="inner").set_index("sample_id")
   164   1939.9 MiB      0.0 MiB           1       assert sum(combined.columns.str.contains("PC")) == 0
   165                                             
   166                                             # compute GRM and get only samples that are represented in the GRM (it should be everything, but this is just to avoid errors)
   167                                             # GRM is in the order of minor_allele_counts_samples (N x N)
   168   7341.2 MiB   5401.3 MiB           1       grm, minor_allele_counts_samples = read_in_matrix_compute_grm("data/minor_allele_counts.npz", combined.index.values)
   169   7341.5 MiB      0.3 MiB           1       combined = combined.loc[minor_allele_counts_samples, :]
   170                                             
   171   7341.5 MiB      0.0 MiB           1       scaler = StandardScaler()
   172   7341.5 MiB      0.0 MiB           1       pca = PCA(n_components=5)
   173   7351.8 MiB     10.3 MiB           1       pca.fit(scaler.fit_transform(grm))
   174                                         
   175   7351.8 MiB      0.0 MiB           1       print(f"Explained variance ratios of 5 principal components: {pca.explained_variance_ratio_}")
   176   7351.8 MiB      0.0 MiB           1       eigenvec = pca.components_.T
   177   7351.8 MiB      0.0 MiB           1       eigenvec_df = pd.DataFrame(eigenvec)
   178   7351.8 MiB      0.0 MiB           1       eigenvec_df.index = minor_allele_counts_samples
   179                                             
   180                                             # combine with eigevectors, then separate the phenotypes
   181   7351.9 MiB      0.0 MiB           1       combined = combined.merge(eigenvec_df, left_index=True, right_index=True)
   182   7351.9 MiB      0.0 MiB           1       if binary:
   183   7351.9 MiB      0.0 MiB           1           y = combined["phenotype"].values
   184   7351.9 MiB      0.0 MiB           1           del combined["phenotype"]
   185                                             
   186   8243.3 MiB    891.4 MiB           1       X = scaler.fit_transform(combined.values)
   187                                             
   188                                             # fit a regression model on the downselected data (only variants with non-zero coefficients and significant p-values after FDR)
   189   8243.3 MiB      0.0 MiB           1       if binary:
   190   8243.3 MiB      0.0 MiB           2           model = LogisticRegressionCV(Cs=np.logspace(-6, 6, 13), 
   191   8243.3 MiB      0.0 MiB           1                                        cv=5,
   192   8243.3 MiB      0.0 MiB           1                                        penalty='l2',
   193   8243.3 MiB      0.0 MiB           1                                        max_iter=10000, 
   194   8243.3 MiB      0.0 MiB           1                                        multi_class='ovr',
   195   8243.3 MiB      0.0 MiB           1                                        scoring='neg_log_loss',
   196   8243.3 MiB      0.0 MiB           1                                        class_weight='balanced'
   197                                                                             )
   198                                             else:
   199                                                 model = RidgeCV(alphas=np.logspace(-6, 6, 13),
   200                                                                 cv=5,
   201                                                                 max_iter=10000,
   202                                                                 scoring='neg_root_mean_squared_error'
   203                                                                )
   204                                             
   205                                             # fit and save the baseline model if you want to make more predictions later
   206   9134.7 MiB    891.4 MiB           1       model.fit(X, y)
   207   9134.7 MiB      0.0 MiB           1       if binary:
   208   9134.7 MiB      0.0 MiB           1           print(f"    Regularization parameter: {model.C_[0]}")
   209   9134.7 MiB      0.0 MiB           1           pickle.dump(model, open(os.path.join(out_dir, drug, 'logReg_model'),'wb'))
   210                                             else:
   211                                                 print(f"    Regularization parameter: {model.alpha_}")
   212                                                 pickle.dump(model, open(os.path.join(out_dir, drug, 'linReg_model'),'wb'))
   213                                         
   214                                         
   215                                             # get the summary stats for the overall model
   216   8244.4 MiB   -890.3 MiB           1       model_outputs = generate_model_output(X, y, model, binary=True, print_thresh=True)
   217   8244.4 MiB      0.0 MiB           1       model_outputs["BS"] = 0
   218                                             
   219                                         #     # next, perform bootstrapping with 1000 replicates
   220                                         #     print(f"Bootstrapping the summary model with {num_bootstrap} replicates")
   221                                         #     for i in range(num_bootstrap):
   222                                         
   223                                         #         # randomly draw sample indices
   224                                         #         sample_idx = np.random.choice(np.arange(0, len(y)), size=len(y), replace=True)
   225                                         
   226                                         #         # get the X and y matrices
   227                                         #         X_bs = X[sample_idx, :]
   228                                         #         y_bs = y[sample_idx]
   229                                         
   230                                         #         if binary:
   231                                         #             bs_model = LogisticRegression(C=model.C_[0], penalty='l2', max_iter=10000, multi_class='ovr', class_weight='balanced')
   232                                         #         else:
   233                                         #             bs_model = Ridge(alpha=model.alpha_, max_iter=10000)
   234                                                 
   235                                         #         bs_model.fit(X_bs, y_bs)
   236                                         #         summary = generate_model_output(X_bs, y_bs, bs_model, binary=binary, print_thresh=False)
   237                                         #         summary["BS"] = 1
   238                                         #         model_outputs = pd.concat([model_outputs, summary], axis=0)
   239                                                 
   240                                         #         if i % (num_bootstrap / 10) == 0:
   241                                         #             print(i)
   242                                         
   243   8244.4 MiB      0.0 MiB           1       return pd.DataFrame(model_outputs)


