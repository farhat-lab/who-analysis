Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102     54.8 MiB     54.8 MiB           1   @profile(stream=mem_log)
   103                                         def read_in_data():
   104                                                 
   105                                             # first get all the genotype files associated with the drug
   106     54.8 MiB      0.0 MiB           1       geno_files = []
   107                                         
   108     54.8 MiB      0.0 MiB           3       for subdir in os.listdir(os.path.join(genos_dir, f"drug_name={drug}")):
   109                                         
   110                                                 # subdirectory (tiers)
   111     54.8 MiB      0.0 MiB           2           full_subdir = os.path.join(genos_dir, f"drug_name={drug}", subdir)
   112                                         
   113                                                 # the last character is the tier number
   114     54.8 MiB      0.0 MiB           2           if full_subdir[-1] in tiers_lst:
   115     54.8 MiB      0.0 MiB           3               for fName in os.listdir(full_subdir):
   116     54.8 MiB      0.0 MiB           2                   if "run" in fName:
   117     54.8 MiB      0.0 MiB           2                       geno_files.append(os.path.join(full_subdir, fName))
   118                                         
   119     54.8 MiB      0.0 MiB           1       print(f"    {len(df_phenos)} samples with phenotypes and {len(geno_files)} files with genotypes.")
   120                                         
   121     54.8 MiB      0.0 MiB           1       dfs_lst = []
   122   2069.3 MiB      0.0 MiB           3       for i, fName in enumerate(geno_files):
   123                                         
   124                                                 # print(f"Reading in genotypes dataframe {i+1}/{len(geno_files)}")
   125                                                 # read in the dataframe
   126   1509.0 MiB    710.3 MiB           2           df = pd.read_csv(fName)
   127                                         
   128                                                 # get only genotypes for samples that have a phenotype
   129   2069.3 MiB   1304.2 MiB           2           df_avail_isolates = df.loc[df.sample_id.isin(df_phenos.sample_id)]
   130                                         
   131                                                 # keep all variants
   132   2069.3 MiB      0.0 MiB           2           if synonymous:
   133   2069.3 MiB      0.0 MiB           2               dfs_lst.append(df_avail_isolates)
   134                                                 else:
   135                                                     # P = coding variants, C = synonymous or upstream variants (get only upstream variants by getting only negative positions), and N = non-coding variants on rrs/rrl
   136                                                     # deletion does not contain the p/c/n prefix
   137                                                     # synonymous variants = synonymous, change in start codon that produces V instead of M, and changes in stop codon that preserve stop
   138                                                     dfs_lst.append(df_avail_isolates.query("predicted_effect not in ['synonymous_variant', 'stop_retained_variant', 'initiator_codon_variant']"))        
   139                                         
   140                                         
   141                                             # possible to have duplicated entries because they have different predicted effects
   142                                             # example: Met1fs is present in two lines because it has 2 predicted effects: frameshift and start lost
   143                                             # sort the dataframe by inverse, which keeps start_lost before frameshift, then drop_duplicates. 
   144   3022.7 MiB    953.5 MiB           1       df_model = pd.concat(dfs_lst)
   145   3837.4 MiB    814.7 MiB           1       return df_model.sort_values("predicted_effect", ascending=False).drop_duplicates(subset=["sample_id", "resolved_symbol", "variant_category", "variant_binary_status", "variant_allele_frequency"], keep="first").reset_index(drop=True)


