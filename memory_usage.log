Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   103     58.3 MiB     58.3 MiB           1   @profile(stream=mem_log)
   104                                         def read_in_data():
   105                                                 
   106                                             # first get all the genotype files associated with the drug
   107     58.3 MiB      0.0 MiB           1       geno_files = []
   108                                         
   109     58.3 MiB      0.0 MiB           3       for subdir in os.listdir(os.path.join(genos_dir, f"drug_name={drug}")):
   110                                         
   111                                                 # subdirectory (tiers)
   112     58.3 MiB      0.0 MiB           2           full_subdir = os.path.join(genos_dir, f"drug_name={drug}", subdir)
   113                                         
   114                                                 # the last character is the tier number
   115     58.3 MiB      0.0 MiB           2           if full_subdir[-1] in tiers_lst:
   116     58.3 MiB      0.0 MiB           8               for fName in os.listdir(full_subdir):
   117     58.3 MiB      0.0 MiB           6                   if "run" in fName:
   118     58.3 MiB      0.0 MiB           6                       geno_files.append(os.path.join(full_subdir, fName))
   119                                         
   120     58.3 MiB      0.0 MiB           1       print(f"    {len(df_phenos)} samples with phenotypes and {len(geno_files)} files with genotypes.")
   121                                         
   122     58.3 MiB      0.0 MiB           1       dfs_lst = []
   123   6410.6 MiB      0.0 MiB           7       for i, fName in enumerate(geno_files):
   124                                         
   125                                                 # print(f"Reading in genotypes dataframe {i+1}/{len(geno_files)}")
   126                                                 # read in the dataframe
   127   5263.0 MiB    529.9 MiB           6           df = pd.read_csv(fName)
   128                                         
   129                                                 # get only genotypes for samples that have a phenotype
   130   6410.6 MiB   5822.3 MiB           6           df_avail_isolates = df.loc[df.sample_id.isin(df_phenos.sample_id)]
   131                                         
   132                                                 # keep all variants
   133   6410.6 MiB      0.0 MiB           6           if synonymous:
   134   6410.6 MiB      0.0 MiB           6               dfs_lst.append(df_avail_isolates)
   135                                                 else:
   136                                                     # P = coding variants, C = synonymous or upstream variants (get only upstream variants by getting only negative positions), and N = non-coding variants on rrs/rrl
   137                                                     # deletion does not contain the p/c/n prefix
   138                                                     # synonymous variants = synonymous, change in start codon that produces V instead of M, and changes in stop codon that preserve stop
   139                                                     dfs_lst.append(df_avail_isolates.query("predicted_effect not in ['synonymous_variant', 'stop_retained_variant', 'initiator_codon_variant']"))        
   140                                         
   141                                         
   142                                             # possible to have duplicated entries because they have different predicted effects
   143                                             # example: Met1fs is present in two lines because it has 2 predicted effects: frameshift and start lost
   144                                             # sort the dataframe by inverse, which keeps start_lost before frameshift, then drop_duplicates. 
   145  11704.7 MiB   5294.2 MiB           1       df_model = pd.concat(dfs_lst)
   146  16337.5 MiB   4632.8 MiB           1       return df_model.sort_values("predicted_effect", ascending=False).drop_duplicates(subset=["sample_id", "resolved_symbol", "variant_category", "variant_binary_status", "variant_allele_frequency"], keep="first").reset_index(drop=True)


Filename: /home/sak0914/who-analysis/01_make_model_inputs.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   154   4723.2 MiB   4723.2 MiB           1   @profile(stream=mem_log)
   155                                         def pool_lof_mutations(df):
   156                                             '''
   157                                             resolved_symbol = gene
   158                                             
   159                                             Effect = lof for ALL frameshift, nonsense, loss of start, and large-scale deletion mutations. 
   160                                             
   161                                             This function creates a new column called lof, which is 1 for variants that are lof, 0 for frameshift mutations that are not lof, and nan for variants that
   162                                             couldn't be lof (synonymous, missense, etc.)
   163                                             
   164                                             LOF criteria = loss of start or stop codon, nonsense mutation, single frameshift mutation, large-scale deletion
   165                                             
   166                                             If one of the above criteria (except the frameshift mutation) co-occurs with multiple frameshift mutations in the same sample and gene, then an lof feature will be
   167                                             generated, and the frameshift mutations will remain as additional features. i.e. the LOF will not trump the multiple frameshift mutations. 
   168                                             '''
   169                                             
   170                                             ###### STEP 1: Assign all (sample, gene) pairs with a single frameshift mutation to LOF, and the remaining to not LOF ######
   171                                             
   172                                             # get all frameshift mutations and separate by the number of frameshifts per gene per sample
   173   5503.7 MiB    780.5 MiB           1       frameshift = df.query("predicted_effect == 'frameshift'")
   174                                         
   175                                             # (sample, gene) pairs with a single frameshift mutation are LOF
   176   5507.2 MiB      3.5 MiB           1       lof_single_fs = pd.DataFrame(frameshift.groupby(["sample_id", "resolved_symbol"])["predicted_effect"].count()).query("predicted_effect == 1").reset_index()
   177                                         
   178                                             # already 1 because variant_category is the counts column now
   179   5507.2 MiB      0.0 MiB           1       lof_single_fs.rename(columns={"predicted_effect": "lof"}, inplace=True)
   180                                         
   181                                             # lof column now is 1 for (sample, gene) pairs with only 1 frameshift mutation and 0 for those with multiple frameshift mutations
   182   5602.8 MiB     95.7 MiB           1       frameshift = frameshift.merge(lof_single_fs, on=["sample_id", "resolved_symbol"], how="outer")
   183   5537.2 MiB    -65.6 MiB           1       frameshift["lof"] = frameshift["lof"].fillna(0)
   184                                         
   185                                             # merge with original dataframe to get the rest of the columns back. predicted_effect is now lof
   186  11559.6 MiB   6022.4 MiB           1       df_with_lof = df.merge(frameshift[["sample_id", "resolved_symbol", "variant_category", "lof"]], on=["sample_id", "resolved_symbol", "variant_category"], how="outer")
   187  11559.6 MiB      0.0 MiB           1       assert len(df) == len(df_with_lof)
   188                                         
   189                                             # value_counts drops all the NaNs when computing
   190  11559.7 MiB      0.0 MiB           1       assert df_with_lof["lof"].value_counts(dropna=True).sum() == len(frameshift)
   191                                         
   192                                             ###### STEP 2: Assign loss of start, stop gained, and large-scale deletion to LOF ######
   193                                         
   194                                             # criteria for lof are: nonsense mutation, loss of start, single frameshift mutation. Get only those satisfying the first two criteria (last done above)
   195  11642.4 MiB   -165.5 MiB           3       df_with_lof.loc[(df_with_lof["variant_category"] == 'deletion') | 
   196  11725.1 MiB     82.8 MiB           2                       (df_with_lof["predicted_effect"].isin(['stop_gained', 'start_lost'])), 'lof'
   197  11559.7 MiB      0.0 MiB           1                      ] = 1
   198                                             
   199                                             # get only variants that are LOF
   200  11559.4 MiB    -83.1 MiB           1       df_lof = df_with_lof.query("lof == 1")
   201                                             
   202                                             ###### STEP 3: COMBINE LOF VARIANTS WITH NON-LOF VARIANTS TO GET A FULL DATAFRAME ######
   203                                             
   204                                             # this dataframe will be slightly smaller than the original because some lof mutations have been pooled
   205                                             
   206                                             # just keep 1 instance because the feature will become just lof. The row that is kept is arbitrary
   207                                             # groupby takes more steps because the rest of the columns need to be gotten again
   208  11578.6 MiB     19.2 MiB           1       df_lof_pooled = df_lof.drop_duplicates(["sample_id", "resolved_symbol"], keep='first')
   209                                             
   210                                             # concatenate the dataframe without LOF variants with the dataframe of pooled LOF variants
   211  17486.5 MiB   5907.9 MiB           1       df_final = pd.concat([df_with_lof.query("lof != 1"), df_lof_pooled], axis=0)
   212                                             
   213                                             # the lof column will now be the variant category to use, so 
   214                                             # 1. replace non-lof frame-shift mutations (value = 0) with NaN 
   215                                             # 2. replace lof variants (value = 1) with the string lof
   216                                             # 3. fill the NaNs (non-lof) with the original variant_category column
   217                                             # 4. rename columns
   218  17486.5 MiB      0.0 MiB           1       df_final["lof"] = df_final["lof"].replace(0, np.nan)
   219  20150.3 MiB   2663.8 MiB           1       df_final["lof"] = df_final["lof"].replace(1, "lof")
   220  17487.1 MiB  -2663.2 MiB           1       df_final["lof"] = df_final["lof"].fillna(df_final["variant_category"])
   221                                             
   222  17487.1 MiB      0.0 MiB           1       assert len(df_final["lof"].unique()) <= len(df_final["variant_category"].unique())
   223  22738.7 MiB   5251.6 MiB           1       return df_final.rename(columns={"variant_category": "variant_category_unpooled", "lof": "variant_category"})


Filename: /home/sak0914/who-analysis/02_regression_with_bootstrap.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    68    773.1 MiB    773.1 MiB           1       @profile(stream=mem_log)
    69                                             def read_in_matrix_compute_grm(fName, model_inputs):
    70   3458.9 MiB   2685.7 MiB           1           minor_allele_counts = sparse.load_npz(fName).todense()
    71                                         
    72                                                 # convert to dataframe
    73   3458.9 MiB      0.0 MiB           1           minor_allele_counts = pd.DataFrame(minor_allele_counts)
    74   3458.9 MiB      0.0 MiB           1           minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    75   3458.9 MiB      0.0 MiB           1           minor_allele_counts = minor_allele_counts.iloc[1:, :]
    76   3459.1 MiB      0.1 MiB           1           minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    77   3458.8 MiB     -0.2 MiB           1           minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    78                                         
    79                                                 # make sample ids the index again
    80   3461.7 MiB      2.9 MiB           1           minor_allele_counts = minor_allele_counts.set_index("sample_id")
    81                                         
    82   3463.8 MiB      2.1 MiB           1           mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    83   3463.8 MiB      0.0 MiB           1           print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    84                                         
    85                                                 # compute GRM using the mino allele counts of only the samples in the model
    86   1877.4 MiB  -1586.4 MiB           1           minor_allele_counts = minor_allele_counts.query("sample_id in @model_inputs.sample_id.values")
    87   5281.7 MiB   3404.3 MiB           1           grm = np.cov(minor_allele_counts.values)
    88                                         
    89   5281.7 MiB      0.0 MiB           1           minor_allele_counts_samples = minor_allele_counts.index.values
    90   4185.3 MiB  -1096.4 MiB           1           del minor_allele_counts
    91   4185.3 MiB      0.0 MiB           1           return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/02_regression_with_bootstrap.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   195   5470.7 MiB   5470.7 MiB           1   @profile(stream=mem_log)
   196                                         def bootstrap_coef():
   197   5470.7 MiB      0.0 MiB           1       coefs = []
   198   5499.7 MiB      0.0 MiB        1001       for i in range(num_bootstrap):
   199                                         
   200                                                 # randomly draw sample indices
   201   5499.7 MiB      0.2 MiB        1000           sample_idx = np.random.choice(np.arange(0, len(y)), size=len(y), replace=True)
   202                                         
   203                                                 # get the X and y matrices
   204   6132.6 MiB 632804.4 MiB        1000           X_bs = X[sample_idx, :]
   205   6132.6 MiB    -46.4 MiB        1000           y_bs = y[sample_idx]
   206                                         
   207   6132.6 MiB    -58.8 MiB        1000           if binary:
   208   6132.6 MiB    -56.3 MiB        1000               bs_model = LogisticRegression(C=model.C_[0], penalty='l2', max_iter=10000, multi_class='ovr', class_weight='balanced')
   209                                                 else:
   210                                                     bs_model = Ridge(alpha=model.alpha_, max_iter=10000)
   211   5499.7 MiB -632856.2 MiB        1000           bs_model.fit(X_bs, y_bs)
   212   5499.7 MiB      0.0 MiB        1000           coefs.append(np.squeeze(bs_model.coef_))
   213                                         
   214   5587.9 MiB     88.1 MiB           1       return pd.DataFrame(coefs)


Filename: /home/sak0914/who-analysis/03_model_analysis.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   142     97.9 MiB     97.9 MiB           1   @profile(stream=mem_log)
   143                                         def run_all(out_dir, drug_abbr, **kwargs):
   144                                             
   145     97.9 MiB      0.0 MiB           1       tiers_lst = kwargs["tiers_lst"]
   146     97.9 MiB      0.0 MiB           1       pheno_category_lst = kwargs["pheno_category_lst"]
   147     97.9 MiB      0.0 MiB           1       model_prefix = kwargs["model_prefix"]
   148     97.9 MiB      0.0 MiB           1       het_mode = kwargs["het_mode"]
   149     97.9 MiB      0.0 MiB           1       synonymous = kwargs["synonymous"]
   150     97.9 MiB      0.0 MiB           1       pool_lof = kwargs["pool_lof"]
   151     97.9 MiB      0.0 MiB           1       AF_thresh = kwargs["AF_thresh"]
   152                                         
   153     97.9 MiB      0.0 MiB           1       num_PCs = kwargs["num_PCs"]
   154     97.9 MiB      0.0 MiB           1       num_bootstrap = kwargs["num_bootstrap"]
   155     97.9 MiB      0.0 MiB           1       alpha = kwargs["alpha"]
   156     97.9 MiB      0.0 MiB           1       binary = kwargs["binary"]
   157                                             
   158                                             # coefficients from L2 regularized regression ("baseline" regression)
   159     98.8 MiB      0.9 MiB           1       coef_df = pd.read_csv(os.path.join(out_dir, "regression_coef.csv"))
   160     99.0 MiB      0.2 MiB           1       coef_df = coef_df.query("coef != 0")
   161                                         
   162                                             # coefficients from bootstrap replicates
   163    165.3 MiB     66.4 MiB           1       bs_df = pd.read_csv(os.path.join(out_dir, "coef_bootstrap.csv"))
   164    154.8 MiB    -10.5 MiB           1       bs_df = bs_df[coef_df["variant"]]
   165                                             
   166                                             # read in all genotypes and phenotypes    
   167    154.8 MiB      0.0 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, "phenos.csv"))
   168                                         
   169                                         #     # add p-values and confidence intervals to the results dataframe
   170                                         #     # if tiers 1 and 2 are included, then compute p-values separately  
   171                                         #     if len(tiers_lst) > 1:
   172                                                 
   173                                         #         tier1_equivalent_path = out_dir.split("tiers")[0] + "tiers=1/phenos" + out_dir.split("phenos")[-1]
   174                                                 
   175                                         #         # if it's not present, then it's because this is a tiers=1+2 model with pooling LOFs. It is possible that the corresponding tiers=1, poolLOF model was not
   176                                         #         # different from the tiers=1 model. So then look for that in this case. 
   177                                         #         if not os.path.isdir(tier1_equivalent_path):
   178                                         #             tier1_equivalent_path = out_dir.split("tiers")[0] + "tiers=1/phenos" + out_dir.split("phenos")[-1].split("_poolLOF")[0]
   179                                                 
   180                                         #         tier1_matrix = pd.read_csv(os.path.join(tier1_equivalent_path, "model_analysis.csv"))
   181                                         #         tier1_variants = tier1_matrix["orig_variant"].values
   182                                                 
   183                                         #         coef_df = get_pvalues_add_ci(coef_df, bs_df, "variant", len(df_phenos), tier1_variants=tier1_variants, alpha=alpha)
   184                                                 
   185                                         #         # all tier 2 genes should have p-values in this case. Tier 1 p-values will be in the corresponding Tier 1 only model
   186                                         #         assert len(coef_df.loc[~coef_df["variant"].isin(tier1_variants) & pd.isnull(coef_df["pval"])]) == 0
   187                                         #     else:
   188                                         #         coef_df = get_pvalues_add_ci(coef_df, bs_df, "variant", len(df_phenos), alpha=alpha)
   189                                             
   190    176.3 MiB     21.5 MiB           1       coef_df = get_pvalues_add_ci(coef_df, bs_df, "variant", len(df_phenos), alpha=alpha)
   191                                                 
   192                                             # Benjamini-Hochberg correction
   193    176.4 MiB      0.1 MiB           1       coef_df = BH_FDR_correction(coef_df)
   194                                         
   195                                             # Bonferroni correction
   196    176.4 MiB      0.0 MiB           1       coef_df["Bonferroni_pval"] = np.min([coef_df["pval"] * len(coef_df), np.ones(len(coef_df))], axis=0)
   197                                         
   198                                             # adjusted p-values are larger so that fewer null hypotheses (coef = 0) are rejected
   199    176.4 MiB      0.0 MiB           1       assert len(coef_df.query("pval > BH_pval")) == 0
   200    176.4 MiB      0.0 MiB           1       assert len(coef_df.query("pval > Bonferroni_pval")) == 0
   201                                         
   202                                             # return all features with non-zero coefficients. Include only variants with nominally significant p-values for tractability
   203                                             # coef_df = coef_df.query("pval < @alpha").sort_values("coef", ascending=False).reset_index(drop=True)
   204    176.9 MiB      0.6 MiB           1       coef_df = find_SNVs_in_current_WHO(coef_df, aa_code_dict, drug_abbr)
   205                                         
   206                                             # convert to odds ratios
   207    176.9 MiB      0.0 MiB           1       if binary:
   208    176.9 MiB      0.0 MiB           1           coef_df["Odds_Ratio"] = np.exp(coef_df["coef"])
   209    176.9 MiB      0.0 MiB           1           coef_df["OR_LB"] = np.exp(coef_df["coef_LB"])
   210    176.9 MiB      0.0 MiB           1           coef_df["OR_UB"] = np.exp(coef_df["coef_UB"])
   211                                          
   212                                             # clean up the dataframe a little -- variant and gene are from the 2021 catalog (redundant with the orig_variant column)
   213    176.9 MiB      0.0 MiB           1       del coef_df["variant"]
   214    176.9 MiB      0.0 MiB           1       del coef_df["gene"]
   215                                             #del coef_df["genome_index"]
   216                                             
   217    176.9 MiB      0.0 MiB           1       return coef_df.drop_duplicates("orig_variant", keep='first').sort_values("coef", ascending=False).reset_index(drop=True)


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    20   1434.6 MiB   1434.6 MiB           1   @profile(stream=mem_log)
    21                                         def read_in_matrix_compute_grm(fName, samples):
    22   4120.0 MiB   2685.4 MiB           1       minor_allele_counts = sparse.load_npz(fName).todense()
    23                                         
    24                                             # convert to dataframe
    25   4120.0 MiB      0.0 MiB           1       minor_allele_counts = pd.DataFrame(minor_allele_counts)
    26   4120.0 MiB      0.0 MiB           1       minor_allele_counts.columns = minor_allele_counts.iloc[0, :]
    27   4120.0 MiB      0.0 MiB           1       minor_allele_counts = minor_allele_counts.iloc[1:, :]
    28   4120.1 MiB      0.1 MiB           1       minor_allele_counts.rename(columns={0:"sample_id"}, inplace=True)
    29   4119.8 MiB     -0.3 MiB           1       minor_allele_counts["sample_id"] = minor_allele_counts["sample_id"].astype(int)
    30                                         
    31                                             # make sample ids the index again
    32   4122.7 MiB      2.9 MiB           1       minor_allele_counts = minor_allele_counts.set_index("sample_id")
    33                                         
    34   4125.3 MiB      2.6 MiB           1       mean_maf = pd.DataFrame(minor_allele_counts.mean(axis=0))
    35   4125.3 MiB      0.0 MiB           1       print(f"Min MAF: {round(mean_maf[0].min(), 2)}, Max MAF: {round(mean_maf[0].max(), 2)}")
    36                                         
    37                                             # compute GRM using the mino allele counts of only the samples in the model
    38   2537.3 MiB  -1588.0 MiB           1       minor_allele_counts = minor_allele_counts.query("sample_id in @samples")
    39   5941.6 MiB   3404.3 MiB           1       grm = np.cov(minor_allele_counts.values)
    40                                         
    41   5941.6 MiB      0.0 MiB           1       minor_allele_counts_samples = minor_allele_counts.index.values
    42   4845.2 MiB  -1096.4 MiB           1       del minor_allele_counts
    43   4845.2 MiB      0.0 MiB           1       return grm, minor_allele_counts_samples


Filename: /home/sak0914/who-analysis/analysis/03_model_metrics.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   130    154.9 MiB    154.9 MiB           1   @profile(stream=mem_log)
   131                                         def compute_downselected_logReg_model(drug, out_dir, binary=True, num_bootstrap=1000):
   132                                             '''
   133                                             This model computes a logistic regression model using the significant predictors from the first model.
   134                                             
   135                                             The original model was used to assign coefficients/odds ratios and p-values. Using the significant predictors (p < 0.05 after FDR), this function
   136                                             builds another L2-penalized logistic regression to compute sensitivity, specificity, AUC, accuracy, and balanced accuracy. 
   137                                             '''
   138                                             
   139                                             # final_analysis file with all significant variants for a drug
   140    156.1 MiB      1.3 MiB           1       res_df = pd.read_csv(os.path.join(out_dir, drug, "final_analysis.csv"))
   141                                             
   142                                             # read in all genotypes and phenotypes and combine into a single dataframe. 
   143                                             # Take the dataframes with the most genotypes and phenotypes represented: tiers=1+2, phenos=ALL
   144                                             # if there are significant LOF variants in res_df, then get the corresponding poolLOF matrix and combine matrices 
   145    157.7 MiB      1.6 MiB           1       df_phenos = pd.read_csv(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "phenos.csv"))
   146                                             
   147    157.9 MiB      0.2 MiB           1       if len(res_df.loc[res_df["orig_variant"].str.contains("lof")]) > 0:
   148                                                 model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   149                                                 model_inputs_poolLOF = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn_poolLOF", "filt_matrix.pkl"))
   150                                                 
   151                                                 # combine dataframes and remove duplicate columns
   152                                                 model_inputs = pd.concat([model_inputs, model_inputs_poolLOF], axis=1)
   153                                                 model_inputs = model_inputs.loc[:,~model_inputs.columns.duplicated()]
   154                                         
   155                                             else:
   156    796.2 MiB    638.3 MiB           1           model_inputs = pd.read_pickle(os.path.join(out_dir, drug, "tiers=1+2/phenos=ALL/dropAF_withSyn", "filt_matrix.pkl"))
   157                                                 
   158                                             # combine into a single dataframe and check that there are no principal components left (because there aren't in df_phenos)
   159   1434.6 MiB    638.3 MiB           1       combined = model_inputs.merge(df_phenos[["sample_id", "phenotype"]], on="sample_id", how="inner").set_index("sample_id")
   160   1434.6 MiB      0.0 MiB           1       assert sum(combined.columns.str.contains("PC")) == 0
   161                                             
   162                                             # compute GRM and get only samples that are represented in the GRM (it should be everything, but this is just to avoid errors)
   163                                             # GRM is in the order of minor_allele_counts_samples (N x N)
   164   4845.2 MiB   3410.6 MiB           1       grm, minor_allele_counts_samples = read_in_matrix_compute_grm("data/minor_allele_counts.npz", combined.index.values)
   165   4845.2 MiB      0.0 MiB           1       combined = combined.loc[minor_allele_counts_samples, :]
   166                                             
   167   4845.2 MiB      0.0 MiB           1       scaler = StandardScaler()
   168   4845.2 MiB      0.0 MiB           1       pca = PCA(n_components=5)
   169   4862.8 MiB     17.6 MiB           1       pca.fit(scaler.fit_transform(grm))
   170                                         
   171   4862.8 MiB      0.0 MiB           1       print(f"Explained variance ratios of 5 principal components: {pca.explained_variance_ratio_}")
   172   4862.8 MiB      0.0 MiB           1       eigenvec = pca.components_.T
   173   4862.8 MiB      0.0 MiB           1       eigenvec_df = pd.DataFrame(eigenvec)
   174   4862.8 MiB      0.0 MiB           1       eigenvec_df.index = minor_allele_counts_samples
   175                                             
   176                                             # combine with eigevectors, then separate the phenotypes
   177   4862.8 MiB      0.0 MiB           1       combined = combined.merge(eigenvec_df, left_index=True, right_index=True)
   178   4862.8 MiB      0.0 MiB           1       if binary:
   179   4862.8 MiB      0.0 MiB           1           y = combined["phenotype"].values
   180   4862.8 MiB      0.0 MiB           1           del combined["phenotype"]
   181                                             
   182   5501.6 MiB    638.7 MiB           1       X = scaler.fit_transform(combined.values)
   183                                             
   184                                             # fit a regression model on the downselected data (only variants with non-zero coefficients and significant p-values after FDR)
   185   5501.6 MiB      0.0 MiB           1       if binary:
   186   5501.6 MiB      0.1 MiB           2           model = LogisticRegressionCV(Cs=np.logspace(-6, 6, 13), 
   187   5501.6 MiB      0.0 MiB           1                                        cv=5,
   188   5501.6 MiB      0.0 MiB           1                                        penalty='l2',
   189   5501.6 MiB      0.0 MiB           1                                        max_iter=10000, 
   190   5501.6 MiB      0.0 MiB           1                                        multi_class='ovr',
   191   5501.6 MiB      0.0 MiB           1                                        scoring='neg_log_loss',
   192   5501.6 MiB      0.0 MiB           1                                        class_weight='balanced'
   193                                                                             )
   194                                             else:
   195                                                 model = RidgeCV(alphas=np.logspace(-6, 6, 13),
   196                                                                 cv=5,
   197                                                                 max_iter=10000,
   198                                                                 scoring='neg_root_mean_squared_error'
   199                                                                )
   200                                             
   201                                             # fit and save the baseline model if you want to make more predictions later
   202   6141.1 MiB    639.4 MiB           1       model.fit(X, y)
   203   6141.1 MiB      0.0 MiB           1       if binary:
   204   6141.1 MiB      0.0 MiB           1           print(f"    Regularization parameter: {model.C_[0]}")
   205   6141.1 MiB      0.0 MiB           1           pickle.dump(model, open(os.path.join(out_dir, drug, 'logReg_model'),'wb'))
   206                                             else:
   207                                                 print(f"    Regularization parameter: {model.alpha_}")
   208                                                 pickle.dump(model, open(os.path.join(out_dir, drug, 'linReg_model'),'wb'))
   209                                         
   210                                         
   211                                             # get the summary stats for the overall model
   212   5503.1 MiB   -637.9 MiB           1       model_outputs = generate_model_output(X, y, model, binary=True, print_thresh=True)
   213   5503.1 MiB      0.0 MiB           1       model_outputs["BS"] = 0
   214                                             
   215                                             # next, perform bootstrapping with 1000 replicates
   216   5503.1 MiB      0.0 MiB           1       print(f"Bootstrapping the summary model with {num_bootstrap} replicates")
   217   6141.3 MiB      0.0 MiB        1001       for i in range(num_bootstrap):
   218                                         
   219                                                 # randomly draw sample indices
   220   6141.3 MiB      0.0 MiB        1000           sample_idx = np.random.choice(np.arange(0, len(y)), size=len(y), replace=True)
   221                                         
   222                                                 # get the X and y matrices
   223   6779.3 MiB 622603.0 MiB        1000           X_bs = X[sample_idx, :]
   224   6779.3 MiB -15390.0 MiB        1000           y_bs = y[sample_idx]
   225                                         
   226   6779.3 MiB -15390.1 MiB        1000           if binary:
   227   6779.3 MiB -15388.1 MiB        1000               bs_model = LogisticRegression(C=model.C_[0], penalty='l2', max_iter=10000, multi_class='ovr', class_weight='balanced')
   228                                                 else:
   229                                                     bs_model = Ridge(alpha=model.alpha_, max_iter=10000)
   230                                                 
   231   6141.3 MiB -637373.6 MiB        1000           bs_model.fit(X_bs, y_bs)
   232   6141.3 MiB      0.1 MiB        1000           summary = generate_model_output(X_bs, y_bs, bs_model, binary=binary, print_thresh=False)
   233   6141.3 MiB      0.0 MiB        1000           summary["BS"] = 1
   234   6141.3 MiB      0.0 MiB        1000           model_outputs = pd.concat([model_outputs, summary], axis=0)
   235                                                 
   236   6141.3 MiB      0.0 MiB        1000           if i % (num_bootstrap / 10) == 0:
   237   6141.3 MiB      0.0 MiB          10               print(i)
   238                                         
   239   6141.3 MiB      0.0 MiB           1       return pd.DataFrame(model_outputs)


